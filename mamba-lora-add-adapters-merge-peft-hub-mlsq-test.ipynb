{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2073977",
   "metadata": {},
   "source": [
    "### Load Pretrained Model\n",
    "Load a pretrained Mamba Model that is compatible with Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db293bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm looking for a new job. I've been working at a company for about a\n"
     ]
    }
   ],
   "source": [
    "from modeling_mamba import MambaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = MambaForCausalLM.from_pretrained('Q-bert/Mamba-130M')\n",
    "tokenizer = AutoTokenizer.from_pretrained('Q-bert/Mamba-130M')\n",
    "\n",
    "text = \"Hi\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids, max_length=20, num_beams=5, no_repeat_ngram_size=2)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'modeling_mamba.MambaForCausalLM'>\n",
      "trainable params: 129135360 || all params: 129135360 || trainable%: 100.0\n",
      "plain None\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "print('plain',print_trainable_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ff2cc",
   "metadata": {},
   "source": [
    "### Load LoRA adapters from the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "a1 = MambaForCausalLM.from_pretrained('mlsquare/exp-lora-ada-1')\n",
    "a2 = MambaForCausalLM.from_pretrained('mlsquare/exp-lora-ada-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6aea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "a1.save_pretrained(\"./mbins/tmp/ada-1\")\n",
    "a2.save_pretrained(\"./mbins/tmp/ada-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501148d",
   "metadata": {},
   "source": [
    "### Average the adapters based on some weight\n",
    "do weighed mean of the adapters of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808886f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 129135360 || all params: 129135360 || trainable%: 100.0\n",
      "base mamba None\n",
      "trainable params: 0 || all params: 129148288 || trainable%: 0.0\n",
      "base mamba None\n",
      "trainable params: 12928 || all params: 129161216 || trainable%: 0.010009196568728495\n",
      "two adapters loaded None\n",
      "trainable params: 12928 || all params: 129174144 || trainable%: 0.010008194828835096\n",
      "two adapters loaded None\n",
      "trainable params: 12928 || all params: 129187072 || trainable%: 0.01000719328943379\n",
      "lora added None\n",
      "trainable params: 0 || all params: 129135360 || trainable%: 0.0\n",
      "merged mamba None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:528: UserWarning: Adapter adapter2 was active which is now deleted. Setting active adapter to adapter3.\n",
      "  warnings.warn(\n",
      "/Users/soma/opt/anaconda3/envs/shane/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:528: UserWarning: Adapter adapter3 was active which is now deleted. Setting active adapter to merged.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftMixedModel, PeftModel\n",
    "\n",
    "\n",
    "base_model = MambaForCausalLM.from_pretrained('Q-bert/Mamba-130M')\n",
    "print('base mamba',print_trainable_parameters(base_model))\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"./mbins/tmp/ada-1\" , \"adapter1\")\n",
    "print('base mamba',print_trainable_parameters(base_model))\n",
    "\n",
    "# note this is just a copy of adapter 1 (only syntax verification)\n",
    "peft_model.load_adapter(\"./mbins/tmp/ada-1\", \"adapter2\")\n",
    "print('two adapters loaded',print_trainable_parameters(peft_model))\n",
    "\n",
    "# note this is just a copy of adapter 1 (only syntax verification)\n",
    "peft_model.load_adapter(\"./mbins/tmp/ada-1\", \"adapter3\")\n",
    "print('two adapters loaded',print_trainable_parameters(peft_model))\n",
    "\n",
    "\n",
    "peft_model.add_weighted_adapter([\"adapter1\", \"adapter2\",\"adapter3\" ], [1.0,1.0,1.0], combination_type=\"linear\", adapter_name=\"merged\")\n",
    "print('lora added',print_trainable_parameters(peft_model))\n",
    "\n",
    "peft_model.delete_adapter(\"adapter1\")\n",
    "peft_model.delete_adapter(\"adapter2\")\n",
    "peft_model.delete_adapter(\"adapter3\")\n",
    "\n",
    "peft_model.merge_and_unload()\n",
    "print('merged mamba',print_trainable_parameters(peft_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e712de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm a newbie to this, but I'm trying to get my head around the\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "output = peft_model.generate(input_ids=input_ids)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shane",
   "language": "python",
   "name": "shane"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
