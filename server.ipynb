{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553eb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mlsquare/mergekit-mamba.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed584747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/yashc/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "\u001b[0m\u001b[01;34mcodeparrot-ds\u001b[0m/           modeling_mamba.py           requirements.txt  \u001b[01;34mwandb\u001b[0m/\r\n",
      "configuration_mamba.py   model_parameters.json       \u001b[01;34mSeshu\u001b[0m/            \u001b[01;34mwts\u001b[0m/\r\n",
      "encoded_inputs_mamba.pt  model_parameters_lora.json  \u001b[01;34mtrained_model\u001b[0m/\r\n",
      "LICENSE                  new_file.yaml               \u001b[01;34mtrial\u001b[0m/\r\n",
      "\u001b[01;34mmamba\u001b[0m/                   \u001b[01;34m__pycache__\u001b[0m/                \u001b[01;34mtrying\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cff9f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ca917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17c52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer,TrainingArguments\n",
    "from huggingface_hub import HfApi, ModelFilter\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import json\n",
    "from configuration_mamba import MambaConfig\n",
    "from modeling_mamba import MambaModel, MambaForCausalLM\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftMixedModel\n",
    "from transformers import DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54c6826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huseinzol05/dummy-mamba-1.4b\n",
      "mesolitica/mamba-1.4B-4096\n",
      "Q-bert/Mamba-130M\n",
      "Q-bert/Mamba-370M\n",
      "Q-bert/Mamba-790M\n",
      "Q-bert/Mamba-1B\n",
      "Q-bert/Mamba-3B\n",
      "s3nh/mamba-1.4b_dolly_instruction_polish\n",
      "ybelkada/test-axolotl-axolotlmambatrainer\n",
      "Q-bert/Mamba-3B-slimpj\n",
      "Q-bert/MambaHermes-3B\n",
      "ZySec-AI/Mamba-2.8B-CyberSec\n",
      "ayoubkirouane/Mamba-Chat-2.8B\n",
      "DeepMount00/Mamba-QA-ITA\n",
      "kuotient/mamba-ko-2.8b\n",
      "Trelis/mamba-2.8b-slimpj-bf16\n",
      "mjschock/mamba-130m\n",
      "mescarda/my_awesome_model\n",
      "DeepMount00/Mamba-QA-ITA-790m\n",
      "mjschock/mamba-370m\n",
      "mjschock/mamba-790m\n",
      "mjschock/mamba-1.4b\n",
      "ArthurZ/small-model\n",
      "ArthurZ/mamba-130m\n",
      "mlsquare/exp_causal_mamba\n",
      "mlsquare/mamba1\n",
      "mlsquare/mamba2\n",
      "mlsquare/mamba3\n",
      "ArthurZ/mamba-2.8b\n",
      "ArthurZ/mamba-370m\n",
      "ArthurZ/mamba-790m\n",
      "ArthurZ/mamba-1.4b\n",
      "ArthurZ/mamba-2.8b-slimpj\n",
      "mjschock/mamba-130m-ppo\n",
      "mlsquare/pico_mamba\n",
      "mlsquare/pico_seshu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mlsquare/exp_causal_mamba',\n",
       " 'mlsquare/mamba1',\n",
       " 'mlsquare/mamba2',\n",
       " 'mlsquare/mamba3',\n",
       " 'mlsquare/pico_mamba',\n",
       " 'mlsquare/pico_seshu']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from peft import PeftMixedModel\n",
    "def get_models_by_organization(org_id):\n",
    "    api = HfApi()\n",
    "    new_filter = ModelFilter(tags=\"mamba\")\n",
    "    models = api.list_models(filter=new_filter)\n",
    "    models_list = []\n",
    "    for i in models:\n",
    "        print(i.modelId)\n",
    "        if org_id in i.modelId:\n",
    "            models_list.append(i.modelId)\n",
    "    return models_list\n",
    "\n",
    "\n",
    "org_id = \"mlsquare\"\n",
    "models = get_models_by_organization(org_id)\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5cc29641",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"small\" : [\"mlsquare/mamba_130M_small_out_proj\", \"mlsquare/mamba_130M_small_dt_proj\", \"mlsquare/mamba_130M_small_x_proj\"],\n",
    "    \"large\" : [\"mlsquare/mamba_130M_large_x_dt_out_proj\"]\n",
    "}\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False): \n",
    "    lm_logits = model(inputs)[0]\n",
    "    labels = inputs.to(lm_logits.device)\n",
    "\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "    return lm_loss\n",
    "\n",
    "    \n",
    "def evaluation(data, model, tokenizer):\n",
    "    val = 0\n",
    "    for i in data:\n",
    "        value = tokenizer.encode(i['tgt'], return_tensors=\"pt\")\n",
    "        val += compute_loss(model, value)\n",
    "        \n",
    "    avg_loss = val / len(data)    \n",
    "    return avg_loss\n",
    "\n",
    "def model_merge_large(adapters, model_path, data, tokenizer):\n",
    "    \n",
    "    model = MambaForCausalLM.from_pretrained(model_path)\n",
    "    model.load_adapter(adapters[\"large\"][0])\n",
    "    result = evaluation(data, model, tokenizer)\n",
    "    return result\n",
    "    \n",
    "def model_merge_small(adapters, model_path, data, tokenizer):\n",
    "\n",
    "    base_model = MambaForCausalLM.from_pretrained(model_path)\n",
    "    peft_model = PeftMixedModel.from_pretrained(base_model, adapters[\"small\"][0])\n",
    "    peft_model.load_adapter(adapters[\"small\"][1], adapter_name=\"1\")\n",
    "    peft_model.load_adapter(adapters[\"small\"][2], adapter_name=\"2\")\n",
    "    peft_model.set_adapter([\"default\", \"1\", \"2\"])\n",
    "    result = evaluation(data, model, tokenizer)\n",
    "    return result\n",
    "    \n",
    "def create_JSON(value):\n",
    "    json_data = json.dumps(value, indent=4)\n",
    "    with open(f\"{value}\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "def get_data(data_path, fraction = 0.1):\n",
    "    data = load_dataset(data_path)['train'].shuffle()\n",
    "    data = data.select(list(range(int(len(data) * fraction))))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b79f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<model>-<PARAMS>-<AdapterComputation>-<target_modules>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_small_out_proj = {\n",
    "    \"model_path\" : \"Q-bert/Mamba-130M\",\n",
    "    \"tokenizer_path\": \"Q-bert/Mamba-130M\",\n",
    "    \"target_modules\": [\"out_proj\"],\n",
    "    \"adapter_path\" : \"mlsquare/mamba-130M-small-out_proj\",\n",
    "    \"data\" : \"mlsquare/samantar1per_cent_merged_with_train_val\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_small_d_proj = {\n",
    "    \"model_path\" : \"Q-bert/Mamba-130M\",\n",
    "    \"tokenizer_path\": \"Q-bert/Mamba-130M\",\n",
    "    \"target_modules\": [\"d_proj\"],\n",
    "    \"adapter_path\" : \"mlsquare/mamba-130M-small-out_proj\",\n",
    "    \"data\" : \"mlsquare/samantar1per_cent_merged_with_train_val\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_small_x_proj = {\n",
    "    \"model_path\" : \"Q-bert/Mamba-130M\",\n",
    "    \"tokenizer_path\": \"Q-bert/Mamba-130M\",\n",
    "    \"target_modules\": [\"x_proj\"],\n",
    "    \"adapter_path\" : \"mlsquare/mamba-130M-small-out_proj\",\n",
    "    \"data\" : \"mlsquare/samantar1per_cent_merged_with_train_val\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ecec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_large_x_d_out_proj = {\n",
    "    \"model_path\" : \"Q-bert/Mamba-130M\",\n",
    "    \"tokenizer_path\": \"Q-bert/Mamba-130M\",\n",
    "    \"target_modules\": [\"x_proj\", \"d_proj\",\"out_proj\"],\n",
    "    \"adapter_path\" : \"mlsquare/mamba-130M-small-out_proj\",\n",
    "    \"data\" : \"mlsquare/samantar1per_cent_merged_with_train_val\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4e918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "pose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
