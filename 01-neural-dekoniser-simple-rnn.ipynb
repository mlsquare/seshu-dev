{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJrAV5+BjXBg6MfU3P+IC2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"zaPaHzVAPWMS","executionInfo":{"status":"ok","timestamp":1715055121911,"user_tz":-330,"elapsed":374,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}}},"outputs":[],"source":["import math\n","from torch import nn, Tensor\n","import torch"]},{"cell_type":"code","source":["words = ['the','and','have', 'that', 'for', 'you', 'with', 'say', 'this', 'they', 'but', 'his', 'from', 'not', 'she', 'as', 'what', 'their', 'can', 'who']\n","print('words',words)\n","\n","vocab = [chr(i) for i in range(ord('a'), ord('z')+1)]\n","vocab.append('0')\n","print('vocab',vocab)\n","\n","letter_to_index = {letter: index for index, letter in enumerate(vocab)}\n","index_to_letter = {index: letter for index, letter in enumerate(vocab)}\n","\n","word_to_index = {word: index for index, word in enumerate(words)}\n","index_to_word = {index: word for index, word in enumerate(words)}\n","\n","\n","\n","#print(letter_to_index)\n","#print(index_to_letter)\n","\n","#print(word_to_index)\n","#print(index_to_word)\n","\n","N = len(words)\n","# includes special end of word character\n","V = len(vocab)\n","# includes end of word character\n","L = max(len(word) for word in words)+1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esgvDr19Pyu4","executionInfo":{"status":"ok","timestamp":1715055063770,"user_tz":-330,"elapsed":505,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}},"outputId":"bb227fe4-3d66-4d38-ac33-70eb1dc8dad0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["words ['the', 'and', 'have', 'that', 'for', 'you', 'with', 'say', 'this', 'they', 'but', 'his', 'from', 'not', 'she', 'as', 'what', 'their', 'can', 'who']\n","vocab ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0']\n"]}]},{"cell_type":"code","source":["# https://github.com/KasperGroesLudvigsen/influenza_transformer/blob/main/positional_encoder.py\n","class PositionalEncoder(nn.Module):\n","    \"\"\"\n","    The authors of the original transformer paper describe very succinctly what\n","    the positional encoding layer does and why it is needed:\n","\n","    \"Since our model contains no recurrence and no convolution, in order for the\n","    model to make use of the order of the sequence, we must inject some\n","    information about the relative or absolute position of the tokens in the\n","    sequence.\" (Vaswani et al, 2017)\n","    Adapted from:\n","    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dropout: float=0.1,\n","        max_seq_len: int=5000,\n","        d_model: int=512,\n","        batch_first: bool=True\n","        ):\n","\n","        \"\"\"\n","        Parameters:\n","            dropout: the dropout rate\n","            max_seq_len: the maximum length of the input sequences\n","            d_model: The dimension of the output of sub-layers in the model\n","                     (Vaswani et al, 2017)\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        self.d_model = d_model\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        self.batch_first = batch_first\n","\n","        # adapted from PyTorch tutorial\n","        position = torch.arange(max_seq_len).unsqueeze(1)\n","\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","\n","        if self.batch_first:\n","            pe = torch.zeros(1, max_seq_len, d_model)\n","\n","            pe[0, :, 0::2] = torch.sin(position * div_term)\n","\n","            pe[0, :, 1::2] = torch.cos(position * div_term)\n","        else:\n","            pe = torch.zeros(max_seq_len, 1, d_model)\n","\n","            pe[:, 0, 0::2] = torch.sin(position * div_term)\n","\n","            pe[:, 0, 1::2] = torch.cos(position * div_term)\n","\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or\n","               [enc_seq_len, batch_size, dim_val]\n","        \"\"\"\n","        if self.batch_first:\n","            x = x + self.pe[:,:x.size(1)]\n","        else:\n","            x = x + self.pe[:x.size(0)]\n","\n","        return self.dropout(x)\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, max_seq_len=L):\n","        super(RNN, self).__init__()\n","        self.input_size = input_size\n","        self.pos_encoder = PositionalEncoder(d_model = input_size, dropout=0.1, max_seq_len=max_seq_len)\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.RNN(input_size, hidden_size, num_layers,batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Initialize hidden state with zeros\n","        # h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n","\n","        #h0 can be improved (h0 = x*toen_embedding)\n","\n","        # Forward pass through RNN\n","        #out, _ = self.rnn(x, h0)\n","        x = x * math.sqrt(self.input_size)\n","        x = self.pos_encoder(x)\n","        out, _ = self.rnn(x)\n","\n","        # Concatenate the output of RNN with y\n","        #out = torch.cat((out[:, -1, :], y.unsqueeze(1)), dim=1)\n","        # out = out[:, -1, :]\n","\n","        # Pass the concatenated output through the fully connected layer\n","        out = self.fc(out)\n","\n","        return out"],"metadata":{"id":"ZrOoXs-bPxEv","executionInfo":{"status":"ok","timestamp":1715055080304,"user_tz":-330,"elapsed":433,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Define the dimensions\n","input_size = N\n","hidden_size = V\n","num_classes = V\n","num_layers = 1\n","\n","# Create an instance of the RNN model\n","model = RNN(input_size, hidden_size, num_layers=num_layers, num_classes=num_classes)\n","\n","# Print the model architecture\n","print(model)\n","\n","# Prepare training set\n","X_train = []\n","Y_train = []\n","\n","for word in words:\n","\n","    chars = list(word)\n","    x = torch.zeros(1, L, N)\n","    y = torch.zeros(1, L)\n","\n","    n = len(chars)\n","    for i in range(L):\n","        x[0, i, word_to_index[word]] = 1\n","        if i < n:\n","            y[0,i] = letter_to_index[chars[i]]\n","        else:\n","            y[0,i] = V-1\n","\n","    X_train.append(x)\n","    Y_train.append(y)\n","\n","# Convert the training set to tensors\n","X_train = torch.cat(X_train, dim=0)\n","Y_train = torch.cat(Y_train, dim=0)\n","\n","# Print the shape of the training set\n","print(\"X_train shape:\", X_train.shape)\n","print(\"Y_train shape:\", Y_train.shape)\n","# prepare training set\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","# Train the model\n","num_epochs = 1000\n","batch_size = 5\n","\n","for epoch in range(num_epochs):\n","    # Shuffle the training data\n","    indices = torch.randperm(X_train.size(0))\n","    X_train_shuffled = X_train[indices]\n","    Y_train_shuffled = Y_train[indices]\n","\n","    # Split the training data into batches\n","    num_batches = X_train.size(0) // batch_size\n","    for batch_idx in range(num_batches):\n","        # Get the batch inputs and targets\n","        start_idx = batch_idx * batch_size\n","        end_idx = start_idx + batch_size\n","        batch_inputs = X_train_shuffled[start_idx:end_idx]\n","        batch_targets = Y_train_shuffled[start_idx:end_idx]\n","\n","        # Forward pass\n","        outputs = model(batch_inputs)\n","\n","        # Compute the loss\n","        loss = criterion(outputs.view(-1, num_classes), batch_targets.view(-1).long())\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print the loss for this epoch\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jV1wAqYjPaGT","executionInfo":{"status":"ok","timestamp":1715055201709,"user_tz":-330,"elapsed":10139,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}},"outputId":"d2359e17-3216-4786-d9cb-1f055c8f85da"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (pos_encoder): PositionalEncoder(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (rnn): RNN(20, 27, batch_first=True)\n","  (fc): Linear(in_features=27, out_features=27, bias=True)\n",")\n","X_train shape: torch.Size([20, 6, 20])\n","Y_train shape: torch.Size([20, 6])\n","Epoch [1/1000], Loss: 3.172258138656616\n","Epoch [2/1000], Loss: 3.123988151550293\n","Epoch [3/1000], Loss: 2.985718011856079\n","Epoch [4/1000], Loss: 2.8663907051086426\n","Epoch [5/1000], Loss: 2.693377733230591\n","Epoch [6/1000], Loss: 2.7134902477264404\n","Epoch [7/1000], Loss: 2.45755672454834\n","Epoch [8/1000], Loss: 2.428528308868408\n","Epoch [9/1000], Loss: 2.397883415222168\n","Epoch [10/1000], Loss: 2.4141294956207275\n","Epoch [11/1000], Loss: 2.3476438522338867\n","Epoch [12/1000], Loss: 2.208467721939087\n","Epoch [13/1000], Loss: 2.197744607925415\n","Epoch [14/1000], Loss: 2.2037367820739746\n","Epoch [15/1000], Loss: 2.093555212020874\n","Epoch [16/1000], Loss: 1.9138263463974\n","Epoch [17/1000], Loss: 2.0887386798858643\n","Epoch [18/1000], Loss: 1.9434293508529663\n","Epoch [19/1000], Loss: 1.7300658226013184\n","Epoch [20/1000], Loss: 1.9495255947113037\n","Epoch [21/1000], Loss: 2.16664981842041\n","Epoch [22/1000], Loss: 1.925277590751648\n","Epoch [23/1000], Loss: 2.039752244949341\n","Epoch [24/1000], Loss: 1.8999381065368652\n","Epoch [25/1000], Loss: 1.7886754274368286\n","Epoch [26/1000], Loss: 1.813596487045288\n","Epoch [27/1000], Loss: 2.098020553588867\n","Epoch [28/1000], Loss: 2.0309855937957764\n","Epoch [29/1000], Loss: 1.894113540649414\n","Epoch [30/1000], Loss: 1.7725127935409546\n","Epoch [31/1000], Loss: 2.121713161468506\n","Epoch [32/1000], Loss: 1.8138657808303833\n","Epoch [33/1000], Loss: 1.8608424663543701\n","Epoch [34/1000], Loss: 1.7719368934631348\n","Epoch [35/1000], Loss: 1.8189148902893066\n","Epoch [36/1000], Loss: 1.7225029468536377\n","Epoch [37/1000], Loss: 1.6812984943389893\n","Epoch [38/1000], Loss: 1.6494885683059692\n","Epoch [39/1000], Loss: 1.7215508222579956\n","Epoch [40/1000], Loss: 1.8363600969314575\n","Epoch [41/1000], Loss: 1.6813864707946777\n","Epoch [42/1000], Loss: 1.6998274326324463\n","Epoch [43/1000], Loss: 1.5509705543518066\n","Epoch [44/1000], Loss: 1.6547986268997192\n","Epoch [45/1000], Loss: 1.514143705368042\n","Epoch [46/1000], Loss: 1.6873743534088135\n","Epoch [47/1000], Loss: 1.6369690895080566\n","Epoch [48/1000], Loss: 1.6143895387649536\n","Epoch [49/1000], Loss: 1.5779831409454346\n","Epoch [50/1000], Loss: 1.7265992164611816\n","Epoch [51/1000], Loss: 1.581749677658081\n","Epoch [52/1000], Loss: 1.573028802871704\n","Epoch [53/1000], Loss: 1.5036109685897827\n","Epoch [54/1000], Loss: 1.4283745288848877\n","Epoch [55/1000], Loss: 1.4705418348312378\n","Epoch [56/1000], Loss: 1.480541706085205\n","Epoch [57/1000], Loss: 1.4607542753219604\n","Epoch [58/1000], Loss: 1.62821364402771\n","Epoch [59/1000], Loss: 1.4282805919647217\n","Epoch [60/1000], Loss: 1.4640873670578003\n","Epoch [61/1000], Loss: 1.4654347896575928\n","Epoch [62/1000], Loss: 1.528935194015503\n","Epoch [63/1000], Loss: 1.4982962608337402\n","Epoch [64/1000], Loss: 1.3810713291168213\n","Epoch [65/1000], Loss: 1.170443058013916\n","Epoch [66/1000], Loss: 1.2494254112243652\n","Epoch [67/1000], Loss: 1.311294674873352\n","Epoch [68/1000], Loss: 1.1460471153259277\n","Epoch [69/1000], Loss: 1.4923418760299683\n","Epoch [70/1000], Loss: 1.184037685394287\n","Epoch [71/1000], Loss: 1.2213854789733887\n","Epoch [72/1000], Loss: 1.3307586908340454\n","Epoch [73/1000], Loss: 1.3207188844680786\n","Epoch [74/1000], Loss: 1.2292723655700684\n","Epoch [75/1000], Loss: 1.3017772436141968\n","Epoch [76/1000], Loss: 1.2990574836730957\n","Epoch [77/1000], Loss: 1.3184027671813965\n","Epoch [78/1000], Loss: 1.3005354404449463\n","Epoch [79/1000], Loss: 1.1851123571395874\n","Epoch [80/1000], Loss: 1.1951426267623901\n","Epoch [81/1000], Loss: 1.0306445360183716\n","Epoch [82/1000], Loss: 1.2172126770019531\n","Epoch [83/1000], Loss: 1.0474743843078613\n","Epoch [84/1000], Loss: 1.041217565536499\n","Epoch [85/1000], Loss: 1.1910004615783691\n","Epoch [86/1000], Loss: 1.0966155529022217\n","Epoch [87/1000], Loss: 1.2047178745269775\n","Epoch [88/1000], Loss: 1.1772277355194092\n","Epoch [89/1000], Loss: 1.003617763519287\n","Epoch [90/1000], Loss: 0.954010546207428\n","Epoch [91/1000], Loss: 1.175432562828064\n","Epoch [92/1000], Loss: 1.1361603736877441\n","Epoch [93/1000], Loss: 1.0281665325164795\n","Epoch [94/1000], Loss: 1.1643717288970947\n","Epoch [95/1000], Loss: 1.1114662885665894\n","Epoch [96/1000], Loss: 1.2169469594955444\n","Epoch [97/1000], Loss: 1.098238468170166\n","Epoch [98/1000], Loss: 0.9375046491622925\n","Epoch [99/1000], Loss: 0.8792692422866821\n","Epoch [100/1000], Loss: 1.1991595029830933\n","Epoch [101/1000], Loss: 0.8340041637420654\n","Epoch [102/1000], Loss: 1.0897074937820435\n","Epoch [103/1000], Loss: 0.9859573245048523\n","Epoch [104/1000], Loss: 1.0853079557418823\n","Epoch [105/1000], Loss: 1.053114891052246\n","Epoch [106/1000], Loss: 1.087105631828308\n","Epoch [107/1000], Loss: 0.8684172034263611\n","Epoch [108/1000], Loss: 1.0582150220870972\n","Epoch [109/1000], Loss: 1.00430428981781\n","Epoch [110/1000], Loss: 1.0885523557662964\n","Epoch [111/1000], Loss: 1.0411229133605957\n","Epoch [112/1000], Loss: 1.0727640390396118\n","Epoch [113/1000], Loss: 1.083943486213684\n","Epoch [114/1000], Loss: 1.1245027780532837\n","Epoch [115/1000], Loss: 0.954010009765625\n","Epoch [116/1000], Loss: 1.0804098844528198\n","Epoch [117/1000], Loss: 1.085906744003296\n","Epoch [118/1000], Loss: 1.1565907001495361\n","Epoch [119/1000], Loss: 0.7956591248512268\n","Epoch [120/1000], Loss: 1.0718655586242676\n","Epoch [121/1000], Loss: 0.8045030832290649\n","Epoch [122/1000], Loss: 0.8921138048171997\n","Epoch [123/1000], Loss: 0.8379619717597961\n","Epoch [124/1000], Loss: 0.9781844019889832\n","Epoch [125/1000], Loss: 0.7876622676849365\n","Epoch [126/1000], Loss: 0.8427831530570984\n","Epoch [127/1000], Loss: 0.8396350741386414\n","Epoch [128/1000], Loss: 0.7488698959350586\n","Epoch [129/1000], Loss: 0.7125229239463806\n","Epoch [130/1000], Loss: 0.859286367893219\n","Epoch [131/1000], Loss: 0.9542789459228516\n","Epoch [132/1000], Loss: 0.9342690706253052\n","Epoch [133/1000], Loss: 0.6839134097099304\n","Epoch [134/1000], Loss: 1.1341172456741333\n","Epoch [135/1000], Loss: 0.8622919321060181\n","Epoch [136/1000], Loss: 0.7614677548408508\n","Epoch [137/1000], Loss: 0.7519740462303162\n","Epoch [138/1000], Loss: 0.7408980131149292\n","Epoch [139/1000], Loss: 0.7102547883987427\n","Epoch [140/1000], Loss: 0.7950857281684875\n","Epoch [141/1000], Loss: 0.8499804735183716\n","Epoch [142/1000], Loss: 0.846888542175293\n","Epoch [143/1000], Loss: 0.8577777147293091\n","Epoch [144/1000], Loss: 0.9777006506919861\n","Epoch [145/1000], Loss: 0.8120295405387878\n","Epoch [146/1000], Loss: 0.7665532231330872\n","Epoch [147/1000], Loss: 0.6695430874824524\n","Epoch [148/1000], Loss: 0.7938758730888367\n","Epoch [149/1000], Loss: 0.6438280940055847\n","Epoch [150/1000], Loss: 0.7074299454689026\n","Epoch [151/1000], Loss: 0.7790817618370056\n","Epoch [152/1000], Loss: 0.7253606915473938\n","Epoch [153/1000], Loss: 0.6351832747459412\n","Epoch [154/1000], Loss: 0.8125045895576477\n","Epoch [155/1000], Loss: 0.9070432782173157\n","Epoch [156/1000], Loss: 0.7401041388511658\n","Epoch [157/1000], Loss: 0.6091604828834534\n","Epoch [158/1000], Loss: 0.7392371296882629\n","Epoch [159/1000], Loss: 0.6645343899726868\n","Epoch [160/1000], Loss: 0.5437172055244446\n","Epoch [161/1000], Loss: 0.6023203730583191\n","Epoch [162/1000], Loss: 0.640051007270813\n","Epoch [163/1000], Loss: 0.6978452205657959\n","Epoch [164/1000], Loss: 0.5334192514419556\n","Epoch [165/1000], Loss: 0.47176313400268555\n","Epoch [166/1000], Loss: 0.5536332726478577\n","Epoch [167/1000], Loss: 0.6283469796180725\n","Epoch [168/1000], Loss: 0.6040699481964111\n","Epoch [169/1000], Loss: 0.6145945191383362\n","Epoch [170/1000], Loss: 0.544654130935669\n","Epoch [171/1000], Loss: 0.6048871278762817\n","Epoch [172/1000], Loss: 0.518505334854126\n","Epoch [173/1000], Loss: 0.5418978929519653\n","Epoch [174/1000], Loss: 0.5652951598167419\n","Epoch [175/1000], Loss: 0.776035487651825\n","Epoch [176/1000], Loss: 0.4443296492099762\n","Epoch [177/1000], Loss: 0.522545576095581\n","Epoch [178/1000], Loss: 0.5237054228782654\n","Epoch [179/1000], Loss: 0.3935355544090271\n","Epoch [180/1000], Loss: 0.5688783526420593\n","Epoch [181/1000], Loss: 0.6028792858123779\n","Epoch [182/1000], Loss: 0.4870986342430115\n","Epoch [183/1000], Loss: 0.6076328754425049\n","Epoch [184/1000], Loss: 0.45090749859809875\n","Epoch [185/1000], Loss: 0.43755850195884705\n","Epoch [186/1000], Loss: 0.5756514072418213\n","Epoch [187/1000], Loss: 0.5913121700286865\n","Epoch [188/1000], Loss: 0.4764763116836548\n","Epoch [189/1000], Loss: 0.5056984424591064\n","Epoch [190/1000], Loss: 0.8216975331306458\n","Epoch [191/1000], Loss: 0.47693318128585815\n","Epoch [192/1000], Loss: 0.5021831393241882\n","Epoch [193/1000], Loss: 0.4922347962856293\n","Epoch [194/1000], Loss: 0.5533168315887451\n","Epoch [195/1000], Loss: 0.5764793753623962\n","Epoch [196/1000], Loss: 0.46935102343559265\n","Epoch [197/1000], Loss: 0.4146173596382141\n","Epoch [198/1000], Loss: 0.5789202451705933\n","Epoch [199/1000], Loss: 0.5115718245506287\n","Epoch [200/1000], Loss: 0.40819787979125977\n","Epoch [201/1000], Loss: 0.5980948805809021\n","Epoch [202/1000], Loss: 0.5593692660331726\n","Epoch [203/1000], Loss: 0.5192567110061646\n","Epoch [204/1000], Loss: 0.7743313908576965\n","Epoch [205/1000], Loss: 0.5813784003257751\n","Epoch [206/1000], Loss: 0.565841555595398\n","Epoch [207/1000], Loss: 0.40458256006240845\n","Epoch [208/1000], Loss: 0.5232769846916199\n","Epoch [209/1000], Loss: 0.5293031334877014\n","Epoch [210/1000], Loss: 0.6198200583457947\n","Epoch [211/1000], Loss: 0.43506309390068054\n","Epoch [212/1000], Loss: 0.6892671585083008\n","Epoch [213/1000], Loss: 0.2953258752822876\n","Epoch [214/1000], Loss: 0.5227598547935486\n","Epoch [215/1000], Loss: 0.5758758783340454\n","Epoch [216/1000], Loss: 0.4402238428592682\n","Epoch [217/1000], Loss: 0.439486563205719\n","Epoch [218/1000], Loss: 0.3078271150588989\n","Epoch [219/1000], Loss: 0.38266706466674805\n","Epoch [220/1000], Loss: 0.397212415933609\n","Epoch [221/1000], Loss: 0.48800981044769287\n","Epoch [222/1000], Loss: 0.32765254378318787\n","Epoch [223/1000], Loss: 0.35962963104248047\n","Epoch [224/1000], Loss: 0.4832752048969269\n","Epoch [225/1000], Loss: 0.36867254972457886\n","Epoch [226/1000], Loss: 0.44907328486442566\n","Epoch [227/1000], Loss: 0.3138027787208557\n","Epoch [228/1000], Loss: 0.561343789100647\n","Epoch [229/1000], Loss: 0.3516143560409546\n","Epoch [230/1000], Loss: 0.3812522888183594\n","Epoch [231/1000], Loss: 0.300861656665802\n","Epoch [232/1000], Loss: 0.335570365190506\n","Epoch [233/1000], Loss: 0.36688581109046936\n","Epoch [234/1000], Loss: 0.41194990277290344\n","Epoch [235/1000], Loss: 0.3017042279243469\n","Epoch [236/1000], Loss: 0.36698049306869507\n","Epoch [237/1000], Loss: 0.40552228689193726\n","Epoch [238/1000], Loss: 0.34302961826324463\n","Epoch [239/1000], Loss: 0.40878772735595703\n","Epoch [240/1000], Loss: 0.48571664094924927\n","Epoch [241/1000], Loss: 0.3205931782722473\n","Epoch [242/1000], Loss: 0.2790202796459198\n","Epoch [243/1000], Loss: 0.3492263853549957\n","Epoch [244/1000], Loss: 0.42930424213409424\n","Epoch [245/1000], Loss: 0.31006601452827454\n","Epoch [246/1000], Loss: 0.290417343378067\n","Epoch [247/1000], Loss: 0.47829344868659973\n","Epoch [248/1000], Loss: 0.3714069128036499\n","Epoch [249/1000], Loss: 0.46387580037117004\n","Epoch [250/1000], Loss: 0.3490087687969208\n","Epoch [251/1000], Loss: 0.34597447514533997\n","Epoch [252/1000], Loss: 0.3574267625808716\n","Epoch [253/1000], Loss: 0.40857064723968506\n","Epoch [254/1000], Loss: 0.35535258054733276\n","Epoch [255/1000], Loss: 0.3615192174911499\n","Epoch [256/1000], Loss: 0.3312479555606842\n","Epoch [257/1000], Loss: 0.23461101949214935\n","Epoch [258/1000], Loss: 0.5100157856941223\n","Epoch [259/1000], Loss: 0.23095230758190155\n","Epoch [260/1000], Loss: 0.3009182810783386\n","Epoch [261/1000], Loss: 0.27325740456581116\n","Epoch [262/1000], Loss: 0.3043328523635864\n","Epoch [263/1000], Loss: 0.5584595203399658\n","Epoch [264/1000], Loss: 0.2506464421749115\n","Epoch [265/1000], Loss: 0.21440055966377258\n","Epoch [266/1000], Loss: 0.7630996704101562\n","Epoch [267/1000], Loss: 0.3038097321987152\n","Epoch [268/1000], Loss: 0.47208085656166077\n","Epoch [269/1000], Loss: 0.3436470925807953\n","Epoch [270/1000], Loss: 0.4718683362007141\n","Epoch [271/1000], Loss: 0.20806346833705902\n","Epoch [272/1000], Loss: 0.25247377157211304\n","Epoch [273/1000], Loss: 0.402183473110199\n","Epoch [274/1000], Loss: 0.5981735587120056\n","Epoch [275/1000], Loss: 0.5989587903022766\n","Epoch [276/1000], Loss: 0.2585381269454956\n","Epoch [277/1000], Loss: 0.20697623491287231\n","Epoch [278/1000], Loss: 0.28745904564857483\n","Epoch [279/1000], Loss: 0.4470835328102112\n","Epoch [280/1000], Loss: 0.28148600459098816\n","Epoch [281/1000], Loss: 0.290900319814682\n","Epoch [282/1000], Loss: 0.34829914569854736\n","Epoch [283/1000], Loss: 0.3922366499900818\n","Epoch [284/1000], Loss: 0.17238156497478485\n","Epoch [285/1000], Loss: 0.16463716328144073\n","Epoch [286/1000], Loss: 0.21763300895690918\n","Epoch [287/1000], Loss: 0.24104765057563782\n","Epoch [288/1000], Loss: 0.27229374647140503\n","Epoch [289/1000], Loss: 0.2762812077999115\n","Epoch [290/1000], Loss: 0.35787299275398254\n","Epoch [291/1000], Loss: 0.28195247054100037\n","Epoch [292/1000], Loss: 0.16717971861362457\n","Epoch [293/1000], Loss: 0.41356155276298523\n","Epoch [294/1000], Loss: 0.18688680231571198\n","Epoch [295/1000], Loss: 0.3897569477558136\n","Epoch [296/1000], Loss: 0.1553288847208023\n","Epoch [297/1000], Loss: 0.5789697766304016\n","Epoch [298/1000], Loss: 0.3152681589126587\n","Epoch [299/1000], Loss: 0.23932814598083496\n","Epoch [300/1000], Loss: 0.36553046107292175\n","Epoch [301/1000], Loss: 0.4524836540222168\n","Epoch [302/1000], Loss: 0.16669544577598572\n","Epoch [303/1000], Loss: 0.6651030778884888\n","Epoch [304/1000], Loss: 0.16290393471717834\n","Epoch [305/1000], Loss: 0.20797981321811676\n","Epoch [306/1000], Loss: 0.16324613988399506\n","Epoch [307/1000], Loss: 0.2341909110546112\n","Epoch [308/1000], Loss: 0.24353154003620148\n","Epoch [309/1000], Loss: 0.33558371663093567\n","Epoch [310/1000], Loss: 0.2085513025522232\n","Epoch [311/1000], Loss: 0.46860751509666443\n","Epoch [312/1000], Loss: 0.48337316513061523\n","Epoch [313/1000], Loss: 0.42520415782928467\n","Epoch [314/1000], Loss: 0.19424885511398315\n","Epoch [315/1000], Loss: 0.3647236227989197\n","Epoch [316/1000], Loss: 0.2533911168575287\n","Epoch [317/1000], Loss: 0.3686678111553192\n","Epoch [318/1000], Loss: 0.2788326144218445\n","Epoch [319/1000], Loss: 0.19685965776443481\n","Epoch [320/1000], Loss: 0.23350423574447632\n","Epoch [321/1000], Loss: 0.31150415539741516\n","Epoch [322/1000], Loss: 0.14541299641132355\n","Epoch [323/1000], Loss: 0.1186428889632225\n","Epoch [324/1000], Loss: 0.21688461303710938\n","Epoch [325/1000], Loss: 0.22434838116168976\n","Epoch [326/1000], Loss: 0.13603875041007996\n","Epoch [327/1000], Loss: 0.16334091126918793\n","Epoch [328/1000], Loss: 0.28535598516464233\n","Epoch [329/1000], Loss: 0.16903339326381683\n","Epoch [330/1000], Loss: 0.13160620629787445\n","Epoch [331/1000], Loss: 0.19861434400081635\n","Epoch [332/1000], Loss: 0.6832647323608398\n","Epoch [333/1000], Loss: 0.19237305223941803\n","Epoch [334/1000], Loss: 0.17787018418312073\n","Epoch [335/1000], Loss: 0.18150398135185242\n","Epoch [336/1000], Loss: 0.1428370624780655\n","Epoch [337/1000], Loss: 0.21831734478473663\n","Epoch [338/1000], Loss: 0.3550656735897064\n","Epoch [339/1000], Loss: 0.3019522428512573\n","Epoch [340/1000], Loss: 0.2900158166885376\n","Epoch [341/1000], Loss: 0.37618574500083923\n","Epoch [342/1000], Loss: 0.6505619287490845\n","Epoch [343/1000], Loss: 0.24601206183433533\n","Epoch [344/1000], Loss: 0.30114662647247314\n","Epoch [345/1000], Loss: 0.10189725458621979\n","Epoch [346/1000], Loss: 0.35887792706489563\n","Epoch [347/1000], Loss: 0.21131417155265808\n","Epoch [348/1000], Loss: 0.3156319856643677\n","Epoch [349/1000], Loss: 0.3943880498409271\n","Epoch [350/1000], Loss: 0.2242879420518875\n","Epoch [351/1000], Loss: 0.10422548651695251\n","Epoch [352/1000], Loss: 0.22250273823738098\n","Epoch [353/1000], Loss: 0.19500134885311127\n","Epoch [354/1000], Loss: 0.7977412939071655\n","Epoch [355/1000], Loss: 0.37057188153266907\n","Epoch [356/1000], Loss: 0.134481281042099\n","Epoch [357/1000], Loss: 0.19685004651546478\n","Epoch [358/1000], Loss: 0.20625096559524536\n","Epoch [359/1000], Loss: 0.32107746601104736\n","Epoch [360/1000], Loss: 0.15599356591701508\n","Epoch [361/1000], Loss: 0.07490716874599457\n","Epoch [362/1000], Loss: 0.1380673050880432\n","Epoch [363/1000], Loss: 0.3133324384689331\n","Epoch [364/1000], Loss: 0.2276141345500946\n","Epoch [365/1000], Loss: 0.17702452838420868\n","Epoch [366/1000], Loss: 0.09702744334936142\n","Epoch [367/1000], Loss: 0.3125940263271332\n","Epoch [368/1000], Loss: 0.4759892225265503\n","Epoch [369/1000], Loss: 0.12447600066661835\n","Epoch [370/1000], Loss: 0.31377503275871277\n","Epoch [371/1000], Loss: 0.12808406352996826\n","Epoch [372/1000], Loss: 0.26972946524620056\n","Epoch [373/1000], Loss: 0.14452706277370453\n","Epoch [374/1000], Loss: 0.17315973341464996\n","Epoch [375/1000], Loss: 0.126088947057724\n","Epoch [376/1000], Loss: 0.45659139752388\n","Epoch [377/1000], Loss: 0.14963792264461517\n","Epoch [378/1000], Loss: 0.15598613023757935\n","Epoch [379/1000], Loss: 0.1596459448337555\n","Epoch [380/1000], Loss: 0.28238344192504883\n","Epoch [381/1000], Loss: 0.15805907547473907\n","Epoch [382/1000], Loss: 0.11115007847547531\n","Epoch [383/1000], Loss: 0.1039026752114296\n","Epoch [384/1000], Loss: 0.16251222789287567\n","Epoch [385/1000], Loss: 0.1298961490392685\n","Epoch [386/1000], Loss: 0.09111686795949936\n","Epoch [387/1000], Loss: 0.12173495441675186\n","Epoch [388/1000], Loss: 0.3733302354812622\n","Epoch [389/1000], Loss: 0.25314226746559143\n","Epoch [390/1000], Loss: 0.12655137479305267\n","Epoch [391/1000], Loss: 0.36111292243003845\n","Epoch [392/1000], Loss: 0.5293481349945068\n","Epoch [393/1000], Loss: 0.11104623228311539\n","Epoch [394/1000], Loss: 0.13537131249904633\n","Epoch [395/1000], Loss: 0.13647037744522095\n","Epoch [396/1000], Loss: 0.39234620332717896\n","Epoch [397/1000], Loss: 0.20702916383743286\n","Epoch [398/1000], Loss: 0.15604907274246216\n","Epoch [399/1000], Loss: 0.09712127596139908\n","Epoch [400/1000], Loss: 0.20475392043590546\n","Epoch [401/1000], Loss: 0.1552797257900238\n","Epoch [402/1000], Loss: 0.21146129071712494\n","Epoch [403/1000], Loss: 0.1891138255596161\n","Epoch [404/1000], Loss: 0.38756808638572693\n","Epoch [405/1000], Loss: 0.07926776260137558\n","Epoch [406/1000], Loss: 0.1091657504439354\n","Epoch [407/1000], Loss: 0.27600327134132385\n","Epoch [408/1000], Loss: 0.09549260139465332\n","Epoch [409/1000], Loss: 0.20126725733280182\n","Epoch [410/1000], Loss: 0.12911449372768402\n","Epoch [411/1000], Loss: 0.1060623750090599\n","Epoch [412/1000], Loss: 0.49035072326660156\n","Epoch [413/1000], Loss: 0.1338207870721817\n","Epoch [414/1000], Loss: 0.19488194584846497\n","Epoch [415/1000], Loss: 0.23281879723072052\n","Epoch [416/1000], Loss: 0.08554477244615555\n","Epoch [417/1000], Loss: 0.16709211468696594\n","Epoch [418/1000], Loss: 0.08422470837831497\n","Epoch [419/1000], Loss: 0.20478086173534393\n","Epoch [420/1000], Loss: 0.19067545235157013\n","Epoch [421/1000], Loss: 0.1943032145500183\n","Epoch [422/1000], Loss: 0.07935504615306854\n","Epoch [423/1000], Loss: 0.1341545283794403\n","Epoch [424/1000], Loss: 0.11169607937335968\n","Epoch [425/1000], Loss: 0.09783053398132324\n","Epoch [426/1000], Loss: 0.5218724608421326\n","Epoch [427/1000], Loss: 0.13264404237270355\n","Epoch [428/1000], Loss: 0.07579030096530914\n","Epoch [429/1000], Loss: 0.08529984951019287\n","Epoch [430/1000], Loss: 0.07724490761756897\n","Epoch [431/1000], Loss: 0.08405932784080505\n","Epoch [432/1000], Loss: 0.24317556619644165\n","Epoch [433/1000], Loss: 0.18982483446598053\n","Epoch [434/1000], Loss: 0.25934818387031555\n","Epoch [435/1000], Loss: 0.3816376030445099\n","Epoch [436/1000], Loss: 0.15873652696609497\n","Epoch [437/1000], Loss: 0.427150160074234\n","Epoch [438/1000], Loss: 0.0916229784488678\n","Epoch [439/1000], Loss: 0.10854010283946991\n","Epoch [440/1000], Loss: 0.24095840752124786\n","Epoch [441/1000], Loss: 0.09037026017904282\n","Epoch [442/1000], Loss: 0.20654171705245972\n","Epoch [443/1000], Loss: 0.07707371562719345\n","Epoch [444/1000], Loss: 0.35598137974739075\n","Epoch [445/1000], Loss: 0.1239694133400917\n","Epoch [446/1000], Loss: 0.5261848568916321\n","Epoch [447/1000], Loss: 0.07230678200721741\n","Epoch [448/1000], Loss: 0.17359694838523865\n","Epoch [449/1000], Loss: 0.05823029577732086\n","Epoch [450/1000], Loss: 0.21476545929908752\n","Epoch [451/1000], Loss: 0.10083252936601639\n","Epoch [452/1000], Loss: 0.3573387563228607\n","Epoch [453/1000], Loss: 0.13619032502174377\n","Epoch [454/1000], Loss: 0.17348140478134155\n","Epoch [455/1000], Loss: 0.2872277498245239\n","Epoch [456/1000], Loss: 0.32145532965660095\n","Epoch [457/1000], Loss: 0.09640724956989288\n","Epoch [458/1000], Loss: 0.05491551384329796\n","Epoch [459/1000], Loss: 0.0707445964217186\n","Epoch [460/1000], Loss: 0.35155951976776123\n","Epoch [461/1000], Loss: 0.05380816012620926\n","Epoch [462/1000], Loss: 0.47587859630584717\n","Epoch [463/1000], Loss: 0.0978761538863182\n","Epoch [464/1000], Loss: 0.08148586004972458\n","Epoch [465/1000], Loss: 0.07248184084892273\n","Epoch [466/1000], Loss: 0.07880749553442001\n","Epoch [467/1000], Loss: 0.09169532358646393\n","Epoch [468/1000], Loss: 0.14111869037151337\n","Epoch [469/1000], Loss: 0.2766674757003784\n","Epoch [470/1000], Loss: 0.26750448346138\n","Epoch [471/1000], Loss: 0.17077915370464325\n","Epoch [472/1000], Loss: 0.05151192471385002\n","Epoch [473/1000], Loss: 0.18122492730617523\n","Epoch [474/1000], Loss: 0.3609396517276764\n","Epoch [475/1000], Loss: 0.22238700091838837\n","Epoch [476/1000], Loss: 0.12028096616268158\n","Epoch [477/1000], Loss: 0.24376119673252106\n","Epoch [478/1000], Loss: 0.3011864721775055\n","Epoch [479/1000], Loss: 0.04199524223804474\n","Epoch [480/1000], Loss: 0.2767985463142395\n","Epoch [481/1000], Loss: 0.08078519254922867\n","Epoch [482/1000], Loss: 0.09352701157331467\n","Epoch [483/1000], Loss: 0.4737858474254608\n","Epoch [484/1000], Loss: 0.28895214200019836\n","Epoch [485/1000], Loss: 0.19480271637439728\n","Epoch [486/1000], Loss: 0.3284936249256134\n","Epoch [487/1000], Loss: 0.06424758583307266\n","Epoch [488/1000], Loss: 0.052021581679582596\n","Epoch [489/1000], Loss: 0.06551757454872131\n","Epoch [490/1000], Loss: 0.08329718559980392\n","Epoch [491/1000], Loss: 0.11373703926801682\n","Epoch [492/1000], Loss: 0.20676152408123016\n","Epoch [493/1000], Loss: 0.0603882297873497\n","Epoch [494/1000], Loss: 0.048734262585639954\n","Epoch [495/1000], Loss: 0.0920320525765419\n","Epoch [496/1000], Loss: 0.046064428985118866\n","Epoch [497/1000], Loss: 0.08139941841363907\n","Epoch [498/1000], Loss: 0.08460958302021027\n","Epoch [499/1000], Loss: 0.196049764752388\n","Epoch [500/1000], Loss: 0.09524595737457275\n","Epoch [501/1000], Loss: 0.2506258189678192\n","Epoch [502/1000], Loss: 0.17658501863479614\n","Epoch [503/1000], Loss: 0.36078864336013794\n","Epoch [504/1000], Loss: 0.07888078689575195\n","Epoch [505/1000], Loss: 0.16712586581707\n","Epoch [506/1000], Loss: 0.10228981822729111\n","Epoch [507/1000], Loss: 0.13722041249275208\n","Epoch [508/1000], Loss: 0.05632822960615158\n","Epoch [509/1000], Loss: 0.1567973494529724\n","Epoch [510/1000], Loss: 0.12987713515758514\n","Epoch [511/1000], Loss: 0.05624323710799217\n","Epoch [512/1000], Loss: 0.3739495575428009\n","Epoch [513/1000], Loss: 0.059832584112882614\n","Epoch [514/1000], Loss: 0.43033257126808167\n","Epoch [515/1000], Loss: 0.06632817536592484\n","Epoch [516/1000], Loss: 0.584018886089325\n","Epoch [517/1000], Loss: 0.06951841711997986\n","Epoch [518/1000], Loss: 0.2963123321533203\n","Epoch [519/1000], Loss: 0.8674217462539673\n","Epoch [520/1000], Loss: 0.09036757797002792\n","Epoch [521/1000], Loss: 0.22172851860523224\n","Epoch [522/1000], Loss: 0.219976007938385\n","Epoch [523/1000], Loss: 0.04995448887348175\n","Epoch [524/1000], Loss: 0.05224032327532768\n","Epoch [525/1000], Loss: 0.10697197169065475\n","Epoch [526/1000], Loss: 0.07131676375865936\n","Epoch [527/1000], Loss: 0.0824354961514473\n","Epoch [528/1000], Loss: 0.06026275455951691\n","Epoch [529/1000], Loss: 0.21432434022426605\n","Epoch [530/1000], Loss: 0.06633581221103668\n","Epoch [531/1000], Loss: 0.20144115388393402\n","Epoch [532/1000], Loss: 0.09045727550983429\n","Epoch [533/1000], Loss: 0.09393219649791718\n","Epoch [534/1000], Loss: 0.08663076907396317\n","Epoch [535/1000], Loss: 0.04838147014379501\n","Epoch [536/1000], Loss: 0.07991684973239899\n","Epoch [537/1000], Loss: 0.19518838822841644\n","Epoch [538/1000], Loss: 0.17805130779743195\n","Epoch [539/1000], Loss: 0.08476801216602325\n","Epoch [540/1000], Loss: 0.035359952598810196\n","Epoch [541/1000], Loss: 0.044950295239686966\n","Epoch [542/1000], Loss: 0.0729828029870987\n","Epoch [543/1000], Loss: 0.07629024982452393\n","Epoch [544/1000], Loss: 0.2695702612400055\n","Epoch [545/1000], Loss: 0.15553095936775208\n","Epoch [546/1000], Loss: 0.07002190500497818\n","Epoch [547/1000], Loss: 0.1288621425628662\n","Epoch [548/1000], Loss: 0.15809708833694458\n","Epoch [549/1000], Loss: 0.2934529185295105\n","Epoch [550/1000], Loss: 0.12530411779880524\n","Epoch [551/1000], Loss: 0.04151095077395439\n","Epoch [552/1000], Loss: 0.042453035712242126\n","Epoch [553/1000], Loss: 0.05534534901380539\n","Epoch [554/1000], Loss: 0.11334796249866486\n","Epoch [555/1000], Loss: 0.05225972458720207\n","Epoch [556/1000], Loss: 0.31049543619155884\n","Epoch [557/1000], Loss: 0.04432106390595436\n","Epoch [558/1000], Loss: 0.2813449501991272\n","Epoch [559/1000], Loss: 0.052189670503139496\n","Epoch [560/1000], Loss: 0.050767309963703156\n","Epoch [561/1000], Loss: 0.05548465996980667\n","Epoch [562/1000], Loss: 0.05950652062892914\n","Epoch [563/1000], Loss: 0.17712636291980743\n","Epoch [564/1000], Loss: 0.04856475442647934\n","Epoch [565/1000], Loss: 0.06600509583950043\n","Epoch [566/1000], Loss: 0.15029999613761902\n","Epoch [567/1000], Loss: 0.0809384435415268\n","Epoch [568/1000], Loss: 0.05175614729523659\n","Epoch [569/1000], Loss: 0.09113241732120514\n","Epoch [570/1000], Loss: 0.14275404810905457\n","Epoch [571/1000], Loss: 0.05292649567127228\n","Epoch [572/1000], Loss: 0.04478063806891441\n","Epoch [573/1000], Loss: 0.09710898250341415\n","Epoch [574/1000], Loss: 0.05109541490674019\n","Epoch [575/1000], Loss: 0.03450370579957962\n","Epoch [576/1000], Loss: 0.1459159553050995\n","Epoch [577/1000], Loss: 0.07357363402843475\n","Epoch [578/1000], Loss: 0.03162959963083267\n","Epoch [579/1000], Loss: 0.11439265310764313\n","Epoch [580/1000], Loss: 0.10257163643836975\n","Epoch [581/1000], Loss: 0.42084142565727234\n","Epoch [582/1000], Loss: 0.07035600394010544\n","Epoch [583/1000], Loss: 0.050825171172618866\n","Epoch [584/1000], Loss: 0.03572331741452217\n","Epoch [585/1000], Loss: 0.04411102086305618\n","Epoch [586/1000], Loss: 0.06971558928489685\n","Epoch [587/1000], Loss: 0.04153360798954964\n","Epoch [588/1000], Loss: 0.042717814445495605\n","Epoch [589/1000], Loss: 0.13986949622631073\n","Epoch [590/1000], Loss: 0.10773441195487976\n","Epoch [591/1000], Loss: 0.16309794783592224\n","Epoch [592/1000], Loss: 0.07434461265802383\n","Epoch [593/1000], Loss: 0.12021348625421524\n","Epoch [594/1000], Loss: 0.03415929898619652\n","Epoch [595/1000], Loss: 0.05056295916438103\n","Epoch [596/1000], Loss: 0.22223640978336334\n","Epoch [597/1000], Loss: 0.08271249383687973\n","Epoch [598/1000], Loss: 0.1840980052947998\n","Epoch [599/1000], Loss: 0.05584259331226349\n","Epoch [600/1000], Loss: 0.03177420049905777\n","Epoch [601/1000], Loss: 0.03675152361392975\n","Epoch [602/1000], Loss: 0.036876726895570755\n","Epoch [603/1000], Loss: 0.3675878942012787\n","Epoch [604/1000], Loss: 0.03622720390558243\n","Epoch [605/1000], Loss: 0.1492307186126709\n","Epoch [606/1000], Loss: 0.04461626708507538\n","Epoch [607/1000], Loss: 0.296408474445343\n","Epoch [608/1000], Loss: 0.07753100991249084\n","Epoch [609/1000], Loss: 0.22593525052070618\n","Epoch [610/1000], Loss: 0.05125747621059418\n","Epoch [611/1000], Loss: 0.1689000278711319\n","Epoch [612/1000], Loss: 0.04622481390833855\n","Epoch [613/1000], Loss: 0.05180898681282997\n","Epoch [614/1000], Loss: 0.03267838805913925\n","Epoch [615/1000], Loss: 0.049795638769865036\n","Epoch [616/1000], Loss: 0.47705018520355225\n","Epoch [617/1000], Loss: 0.26419010758399963\n","Epoch [618/1000], Loss: 0.22633987665176392\n","Epoch [619/1000], Loss: 0.05302579700946808\n","Epoch [620/1000], Loss: 0.07901157438755035\n","Epoch [621/1000], Loss: 0.0349852479994297\n","Epoch [622/1000], Loss: 0.04512859880924225\n","Epoch [623/1000], Loss: 0.17216363549232483\n","Epoch [624/1000], Loss: 0.450563907623291\n","Epoch [625/1000], Loss: 0.3691776692867279\n","Epoch [626/1000], Loss: 0.06202184036374092\n","Epoch [627/1000], Loss: 0.12089972198009491\n","Epoch [628/1000], Loss: 0.053852032870054245\n","Epoch [629/1000], Loss: 0.22596143186092377\n","Epoch [630/1000], Loss: 0.21585769951343536\n","Epoch [631/1000], Loss: 0.10643667727708817\n","Epoch [632/1000], Loss: 0.12863601744174957\n","Epoch [633/1000], Loss: 0.10951978713274002\n","Epoch [634/1000], Loss: 0.3549063503742218\n","Epoch [635/1000], Loss: 0.05277479439973831\n","Epoch [636/1000], Loss: 0.1615183800458908\n","Epoch [637/1000], Loss: 0.0864366963505745\n","Epoch [638/1000], Loss: 0.037554748356342316\n","Epoch [639/1000], Loss: 0.02376781590282917\n","Epoch [640/1000], Loss: 0.23386307060718536\n","Epoch [641/1000], Loss: 0.030042527243494987\n","Epoch [642/1000], Loss: 0.22628754377365112\n","Epoch [643/1000], Loss: 0.3082779347896576\n","Epoch [644/1000], Loss: 0.5845931172370911\n","Epoch [645/1000], Loss: 0.0339723601937294\n","Epoch [646/1000], Loss: 0.04990970715880394\n","Epoch [647/1000], Loss: 0.02735120989382267\n","Epoch [648/1000], Loss: 0.14844107627868652\n","Epoch [649/1000], Loss: 0.1982450783252716\n","Epoch [650/1000], Loss: 0.04462515935301781\n","Epoch [651/1000], Loss: 0.046314138919115067\n","Epoch [652/1000], Loss: 0.11740634590387344\n","Epoch [653/1000], Loss: 0.05130346864461899\n","Epoch [654/1000], Loss: 0.5288780927658081\n","Epoch [655/1000], Loss: 0.044283874332904816\n","Epoch [656/1000], Loss: 0.19697853922843933\n","Epoch [657/1000], Loss: 0.19761469960212708\n","Epoch [658/1000], Loss: 0.1162296012043953\n","Epoch [659/1000], Loss: 0.031104717403650284\n","Epoch [660/1000], Loss: 0.19933180510997772\n","Epoch [661/1000], Loss: 0.03177441284060478\n","Epoch [662/1000], Loss: 0.11803906410932541\n","Epoch [663/1000], Loss: 0.13083958625793457\n","Epoch [664/1000], Loss: 0.049033116549253464\n","Epoch [665/1000], Loss: 0.05763307586312294\n","Epoch [666/1000], Loss: 0.05831269174814224\n","Epoch [667/1000], Loss: 0.05432026460766792\n","Epoch [668/1000], Loss: 0.040188804268836975\n","Epoch [669/1000], Loss: 0.2929622232913971\n","Epoch [670/1000], Loss: 0.2952062487602234\n","Epoch [671/1000], Loss: 0.30524128675460815\n","Epoch [672/1000], Loss: 0.06275936216115952\n","Epoch [673/1000], Loss: 0.22070401906967163\n","Epoch [674/1000], Loss: 0.5428829789161682\n","Epoch [675/1000], Loss: 0.052248772233724594\n","Epoch [676/1000], Loss: 0.04364079609513283\n","Epoch [677/1000], Loss: 0.30732473731040955\n","Epoch [678/1000], Loss: 0.029098115861415863\n","Epoch [679/1000], Loss: 0.1389763057231903\n","Epoch [680/1000], Loss: 0.04135516658425331\n","Epoch [681/1000], Loss: 0.03581253066658974\n","Epoch [682/1000], Loss: 0.05929870158433914\n","Epoch [683/1000], Loss: 0.02807401493191719\n","Epoch [684/1000], Loss: 0.022704847157001495\n","Epoch [685/1000], Loss: 0.05784625560045242\n","Epoch [686/1000], Loss: 0.36909914016723633\n","Epoch [687/1000], Loss: 0.0666448101401329\n","Epoch [688/1000], Loss: 0.022778762504458427\n","Epoch [689/1000], Loss: 0.02415464259684086\n","Epoch [690/1000], Loss: 0.05716632679104805\n","Epoch [691/1000], Loss: 0.14768601953983307\n","Epoch [692/1000], Loss: 0.3920860290527344\n","Epoch [693/1000], Loss: 0.10793229937553406\n","Epoch [694/1000], Loss: 0.026532137766480446\n","Epoch [695/1000], Loss: 0.22814352810382843\n","Epoch [696/1000], Loss: 0.024614272639155388\n","Epoch [697/1000], Loss: 0.025774113833904266\n","Epoch [698/1000], Loss: 0.07459235191345215\n","Epoch [699/1000], Loss: 0.1407286524772644\n","Epoch [700/1000], Loss: 0.07265563309192657\n","Epoch [701/1000], Loss: 0.0643932893872261\n","Epoch [702/1000], Loss: 0.03243600204586983\n","Epoch [703/1000], Loss: 1.3967137336730957\n","Epoch [704/1000], Loss: 0.03848642483353615\n","Epoch [705/1000], Loss: 0.025550473481416702\n","Epoch [706/1000], Loss: 0.3856584429740906\n","Epoch [707/1000], Loss: 0.2262936234474182\n","Epoch [708/1000], Loss: 0.031146962195634842\n","Epoch [709/1000], Loss: 0.03301028907299042\n","Epoch [710/1000], Loss: 0.02319381572306156\n","Epoch [711/1000], Loss: 0.16359224915504456\n","Epoch [712/1000], Loss: 0.028743593022227287\n","Epoch [713/1000], Loss: 0.2074871063232422\n","Epoch [714/1000], Loss: 0.09686741977930069\n","Epoch [715/1000], Loss: 0.1759246289730072\n","Epoch [716/1000], Loss: 0.04162532836198807\n","Epoch [717/1000], Loss: 0.36391013860702515\n","Epoch [718/1000], Loss: 0.05074575915932655\n","Epoch [719/1000], Loss: 0.1867503821849823\n","Epoch [720/1000], Loss: 0.023944176733493805\n","Epoch [721/1000], Loss: 0.02186855860054493\n","Epoch [722/1000], Loss: 0.023465782403945923\n","Epoch [723/1000], Loss: 0.040845390409231186\n","Epoch [724/1000], Loss: 0.022464487701654434\n","Epoch [725/1000], Loss: 0.2261907309293747\n","Epoch [726/1000], Loss: 0.02536039985716343\n","Epoch [727/1000], Loss: 0.028736740350723267\n","Epoch [728/1000], Loss: 0.023898392915725708\n","Epoch [729/1000], Loss: 0.05754435434937477\n","Epoch [730/1000], Loss: 0.17538513243198395\n","Epoch [731/1000], Loss: 0.35813701152801514\n","Epoch [732/1000], Loss: 0.7403227686882019\n","Epoch [733/1000], Loss: 0.2505631744861603\n","Epoch [734/1000], Loss: 0.049875658005476\n","Epoch [735/1000], Loss: 0.043647658079862595\n","Epoch [736/1000], Loss: 0.10919394344091415\n","Epoch [737/1000], Loss: 0.03777693584561348\n","Epoch [738/1000], Loss: 0.22527045011520386\n","Epoch [739/1000], Loss: 0.04334315285086632\n","Epoch [740/1000], Loss: 0.024090267717838287\n","Epoch [741/1000], Loss: 0.027111895382404327\n","Epoch [742/1000], Loss: 0.04130353778600693\n","Epoch [743/1000], Loss: 0.027799565345048904\n","Epoch [744/1000], Loss: 0.02327086590230465\n","Epoch [745/1000], Loss: 0.013941885903477669\n","Epoch [746/1000], Loss: 0.03522428497672081\n","Epoch [747/1000], Loss: 0.02529786340892315\n","Epoch [748/1000], Loss: 0.10692764818668365\n","Epoch [749/1000], Loss: 0.030152691528201103\n","Epoch [750/1000], Loss: 0.027704525738954544\n","Epoch [751/1000], Loss: 0.25075045228004456\n","Epoch [752/1000], Loss: 0.020455244928598404\n","Epoch [753/1000], Loss: 0.07860253751277924\n","Epoch [754/1000], Loss: 0.24129828810691833\n","Epoch [755/1000], Loss: 0.017901849001646042\n","Epoch [756/1000], Loss: 0.16276870667934418\n","Epoch [757/1000], Loss: 0.04185809940099716\n","Epoch [758/1000], Loss: 0.028236307203769684\n","Epoch [759/1000], Loss: 0.23879830539226532\n","Epoch [760/1000], Loss: 0.027053233236074448\n","Epoch [761/1000], Loss: 0.04504746198654175\n","Epoch [762/1000], Loss: 0.09509098529815674\n","Epoch [763/1000], Loss: 0.03949316591024399\n","Epoch [764/1000], Loss: 0.02264135517179966\n","Epoch [765/1000], Loss: 0.03227154165506363\n","Epoch [766/1000], Loss: 0.016638794913887978\n","Epoch [767/1000], Loss: 0.030397208407521248\n","Epoch [768/1000], Loss: 0.48017919063568115\n","Epoch [769/1000], Loss: 0.177908256649971\n","Epoch [770/1000], Loss: 0.17860576510429382\n","Epoch [771/1000], Loss: 0.05691731348633766\n","Epoch [772/1000], Loss: 0.033964693546295166\n","Epoch [773/1000], Loss: 0.02723691239953041\n","Epoch [774/1000], Loss: 0.18247203528881073\n","Epoch [775/1000], Loss: 0.05469750240445137\n","Epoch [776/1000], Loss: 0.035899363458156586\n","Epoch [777/1000], Loss: 0.07104816287755966\n","Epoch [778/1000], Loss: 0.031910452991724014\n","Epoch [779/1000], Loss: 0.039885345846414566\n","Epoch [780/1000], Loss: 0.021205652505159378\n","Epoch [781/1000], Loss: 0.07660863548517227\n","Epoch [782/1000], Loss: 0.06288047134876251\n","Epoch [783/1000], Loss: 0.07140317559242249\n","Epoch [784/1000], Loss: 0.0652690902352333\n","Epoch [785/1000], Loss: 0.025297554209828377\n","Epoch [786/1000], Loss: 0.02646288461983204\n","Epoch [787/1000], Loss: 0.04573628678917885\n","Epoch [788/1000], Loss: 0.04197751358151436\n","Epoch [789/1000], Loss: 0.0339084155857563\n","Epoch [790/1000], Loss: 0.19953522086143494\n","Epoch [791/1000], Loss: 0.2211819440126419\n","Epoch [792/1000], Loss: 0.17192690074443817\n","Epoch [793/1000], Loss: 0.15761208534240723\n","Epoch [794/1000], Loss: 0.14685113728046417\n","Epoch [795/1000], Loss: 0.4091269075870514\n","Epoch [796/1000], Loss: 0.01858634687960148\n","Epoch [797/1000], Loss: 0.03473886474967003\n","Epoch [798/1000], Loss: 0.11067506670951843\n","Epoch [799/1000], Loss: 0.06091487035155296\n","Epoch [800/1000], Loss: 0.022631226107478142\n","Epoch [801/1000], Loss: 0.02056995965540409\n","Epoch [802/1000], Loss: 0.031117888167500496\n","Epoch [803/1000], Loss: 0.09523782879114151\n","Epoch [804/1000], Loss: 0.020803360268473625\n","Epoch [805/1000], Loss: 0.03286200389266014\n","Epoch [806/1000], Loss: 0.23400363326072693\n","Epoch [807/1000], Loss: 0.015149692073464394\n","Epoch [808/1000], Loss: 0.5382755994796753\n","Epoch [809/1000], Loss: 0.02659948356449604\n","Epoch [810/1000], Loss: 0.020471982657909393\n","Epoch [811/1000], Loss: 0.5106827616691589\n","Epoch [812/1000], Loss: 0.016444969922304153\n","Epoch [813/1000], Loss: 0.07986555993556976\n","Epoch [814/1000], Loss: 0.12881958484649658\n","Epoch [815/1000], Loss: 0.1714048683643341\n","Epoch [816/1000], Loss: 0.4076085388660431\n","Epoch [817/1000], Loss: 0.013683149591088295\n","Epoch [818/1000], Loss: 0.13351894915103912\n","Epoch [819/1000], Loss: 0.16607967019081116\n","Epoch [820/1000], Loss: 0.018042387440800667\n","Epoch [821/1000], Loss: 0.0238615982234478\n","Epoch [822/1000], Loss: 0.02702866867184639\n","Epoch [823/1000], Loss: 0.03918452188372612\n","Epoch [824/1000], Loss: 0.04280596971511841\n","Epoch [825/1000], Loss: 0.02740504778921604\n","Epoch [826/1000], Loss: 0.14602313935756683\n","Epoch [827/1000], Loss: 0.03279465064406395\n","Epoch [828/1000], Loss: 0.3830046057701111\n","Epoch [829/1000], Loss: 0.01912696100771427\n","Epoch [830/1000], Loss: 0.21249660849571228\n","Epoch [831/1000], Loss: 0.013779648579657078\n","Epoch [832/1000], Loss: 0.03949202597141266\n","Epoch [833/1000], Loss: 0.026696207001805305\n","Epoch [834/1000], Loss: 0.38607123494148254\n","Epoch [835/1000], Loss: 0.023642709478735924\n","Epoch [836/1000], Loss: 0.07525531202554703\n","Epoch [837/1000], Loss: 0.07413378357887268\n","Epoch [838/1000], Loss: 0.03891453891992569\n","Epoch [839/1000], Loss: 0.013987896963953972\n","Epoch [840/1000], Loss: 0.01897422783076763\n","Epoch [841/1000], Loss: 0.021034521982073784\n","Epoch [842/1000], Loss: 0.021029101684689522\n","Epoch [843/1000], Loss: 0.09387902170419693\n","Epoch [844/1000], Loss: 0.047499384731054306\n","Epoch [845/1000], Loss: 0.021317826583981514\n","Epoch [846/1000], Loss: 0.025412673130631447\n","Epoch [847/1000], Loss: 0.021194858476519585\n","Epoch [848/1000], Loss: 0.28960102796554565\n","Epoch [849/1000], Loss: 0.02831183932721615\n","Epoch [850/1000], Loss: 0.020629126578569412\n","Epoch [851/1000], Loss: 0.6075006127357483\n","Epoch [852/1000], Loss: 0.019499940797686577\n","Epoch [853/1000], Loss: 0.019286898896098137\n","Epoch [854/1000], Loss: 0.027761008590459824\n","Epoch [855/1000], Loss: 0.03466128185391426\n","Epoch [856/1000], Loss: 0.13660621643066406\n","Epoch [857/1000], Loss: 0.0258114505559206\n","Epoch [858/1000], Loss: 0.09721041470766068\n","Epoch [859/1000], Loss: 0.0396021232008934\n","Epoch [860/1000], Loss: 0.022209150716662407\n","Epoch [861/1000], Loss: 0.2210303097963333\n","Epoch [862/1000], Loss: 0.2399461269378662\n","Epoch [863/1000], Loss: 0.016456641256809235\n","Epoch [864/1000], Loss: 0.022745240479707718\n","Epoch [865/1000], Loss: 0.5422379970550537\n","Epoch [866/1000], Loss: 0.27378150820732117\n","Epoch [867/1000], Loss: 0.19841799139976501\n","Epoch [868/1000], Loss: 0.08315526694059372\n","Epoch [869/1000], Loss: 0.21861758828163147\n","Epoch [870/1000], Loss: 0.15741395950317383\n","Epoch [871/1000], Loss: 0.2393006682395935\n","Epoch [872/1000], Loss: 0.06606445461511612\n","Epoch [873/1000], Loss: 0.01741800643503666\n","Epoch [874/1000], Loss: 0.032854415476322174\n","Epoch [875/1000], Loss: 0.03267652913928032\n","Epoch [876/1000], Loss: 0.025548452511429787\n","Epoch [877/1000], Loss: 0.017923515290021896\n","Epoch [878/1000], Loss: 0.21830740571022034\n","Epoch [879/1000], Loss: 0.02061324566602707\n","Epoch [880/1000], Loss: 0.03420040383934975\n","Epoch [881/1000], Loss: 0.040142081677913666\n","Epoch [882/1000], Loss: 0.04701908305287361\n","Epoch [883/1000], Loss: 0.017011066898703575\n","Epoch [884/1000], Loss: 0.27617987990379333\n","Epoch [885/1000], Loss: 0.2879653573036194\n","Epoch [886/1000], Loss: 0.22689597308635712\n","Epoch [887/1000], Loss: 0.24988555908203125\n","Epoch [888/1000], Loss: 0.031051063910126686\n","Epoch [889/1000], Loss: 0.22802981734275818\n","Epoch [890/1000], Loss: 0.3066197335720062\n","Epoch [891/1000], Loss: 0.050465550273656845\n","Epoch [892/1000], Loss: 0.493805855512619\n","Epoch [893/1000], Loss: 0.01819419674575329\n","Epoch [894/1000], Loss: 0.248750239610672\n","Epoch [895/1000], Loss: 0.017303837463259697\n","Epoch [896/1000], Loss: 0.1211031898856163\n","Epoch [897/1000], Loss: 0.2579424977302551\n","Epoch [898/1000], Loss: 0.31012117862701416\n","Epoch [899/1000], Loss: 0.07603689283132553\n","Epoch [900/1000], Loss: 0.015306998044252396\n","Epoch [901/1000], Loss: 0.03862126171588898\n","Epoch [902/1000], Loss: 0.1949252188205719\n","Epoch [903/1000], Loss: 0.10623247176408768\n","Epoch [904/1000], Loss: 0.014479088596999645\n","Epoch [905/1000], Loss: 0.037953950464725494\n","Epoch [906/1000], Loss: 0.20408111810684204\n","Epoch [907/1000], Loss: 0.035522863268852234\n","Epoch [908/1000], Loss: 0.018988199532032013\n","Epoch [909/1000], Loss: 0.02361161634325981\n","Epoch [910/1000], Loss: 0.05181553214788437\n","Epoch [911/1000], Loss: 0.05038166791200638\n","Epoch [912/1000], Loss: 0.0354500487446785\n","Epoch [913/1000], Loss: 0.016452958807349205\n","Epoch [914/1000], Loss: 0.09949113428592682\n","Epoch [915/1000], Loss: 0.13940885663032532\n","Epoch [916/1000], Loss: 0.2257266342639923\n","Epoch [917/1000], Loss: 0.2081102728843689\n","Epoch [918/1000], Loss: 0.02305779978632927\n","Epoch [919/1000], Loss: 0.08066686242818832\n","Epoch [920/1000], Loss: 0.19126199185848236\n","Epoch [921/1000], Loss: 0.02713594026863575\n","Epoch [922/1000], Loss: 0.03809589147567749\n","Epoch [923/1000], Loss: 0.01542164571583271\n","Epoch [924/1000], Loss: 0.029046405106782913\n","Epoch [925/1000], Loss: 0.060447923839092255\n","Epoch [926/1000], Loss: 0.014940325170755386\n","Epoch [927/1000], Loss: 0.07706116139888763\n","Epoch [928/1000], Loss: 0.027778595685958862\n","Epoch [929/1000], Loss: 0.027759237214922905\n","Epoch [930/1000], Loss: 0.12716157734394073\n","Epoch [931/1000], Loss: 0.016737518832087517\n","Epoch [932/1000], Loss: 0.03780360519886017\n","Epoch [933/1000], Loss: 0.02376232109963894\n","Epoch [934/1000], Loss: 0.01973625272512436\n","Epoch [935/1000], Loss: 0.014689221046864986\n","Epoch [936/1000], Loss: 0.09217626601457596\n","Epoch [937/1000], Loss: 0.22016923129558563\n","Epoch [938/1000], Loss: 0.28532347083091736\n","Epoch [939/1000], Loss: 0.17701734602451324\n","Epoch [940/1000], Loss: 0.12512066960334778\n","Epoch [941/1000], Loss: 0.09023752063512802\n","Epoch [942/1000], Loss: 0.020912256091833115\n","Epoch [943/1000], Loss: 0.14151635766029358\n","Epoch [944/1000], Loss: 0.019995754584670067\n","Epoch [945/1000], Loss: 0.013560154475271702\n","Epoch [946/1000], Loss: 0.01968410797417164\n","Epoch [947/1000], Loss: 0.018212763592600822\n","Epoch [948/1000], Loss: 0.11382726579904556\n","Epoch [949/1000], Loss: 0.01916162110865116\n","Epoch [950/1000], Loss: 0.02506311610341072\n","Epoch [951/1000], Loss: 0.08897389471530914\n","Epoch [952/1000], Loss: 0.031838417053222656\n","Epoch [953/1000], Loss: 0.119596928358078\n","Epoch [954/1000], Loss: 0.10883279144763947\n","Epoch [955/1000], Loss: 0.016144899651408195\n","Epoch [956/1000], Loss: 0.044621825218200684\n","Epoch [957/1000], Loss: 0.014060218818485737\n","Epoch [958/1000], Loss: 0.025100750848650932\n","Epoch [959/1000], Loss: 0.022859135642647743\n","Epoch [960/1000], Loss: 0.07158304750919342\n","Epoch [961/1000], Loss: 0.03734768182039261\n","Epoch [962/1000], Loss: 0.012764559127390385\n","Epoch [963/1000], Loss: 0.01749868504703045\n","Epoch [964/1000], Loss: 0.19859153032302856\n","Epoch [965/1000], Loss: 0.11719129979610443\n","Epoch [966/1000], Loss: 0.0788273885846138\n","Epoch [967/1000], Loss: 0.38175132870674133\n","Epoch [968/1000], Loss: 0.14692828059196472\n","Epoch [969/1000], Loss: 0.01910579390823841\n","Epoch [970/1000], Loss: 0.19326995313167572\n","Epoch [971/1000], Loss: 0.012086126022040844\n","Epoch [972/1000], Loss: 0.1382860541343689\n","Epoch [973/1000], Loss: 0.059026408940553665\n","Epoch [974/1000], Loss: 0.11051743477582932\n","Epoch [975/1000], Loss: 0.1459733098745346\n","Epoch [976/1000], Loss: 0.3136296570301056\n","Epoch [977/1000], Loss: 0.021799933165311813\n","Epoch [978/1000], Loss: 0.07328470796346664\n","Epoch [979/1000], Loss: 0.012914415448904037\n","Epoch [980/1000], Loss: 0.01419265940785408\n","Epoch [981/1000], Loss: 0.5399804711341858\n","Epoch [982/1000], Loss: 0.02500089816749096\n","Epoch [983/1000], Loss: 0.07267116755247116\n","Epoch [984/1000], Loss: 0.01984664984047413\n","Epoch [985/1000], Loss: 0.12437979876995087\n","Epoch [986/1000], Loss: 0.30461063981056213\n","Epoch [987/1000], Loss: 0.016219351440668106\n","Epoch [988/1000], Loss: 0.015333124436438084\n","Epoch [989/1000], Loss: 0.16537263989448547\n","Epoch [990/1000], Loss: 0.015448975376784801\n","Epoch [991/1000], Loss: 0.022753547877073288\n","Epoch [992/1000], Loss: 0.02709256298840046\n","Epoch [993/1000], Loss: 0.025555379688739777\n","Epoch [994/1000], Loss: 0.01296465564519167\n","Epoch [995/1000], Loss: 0.02076282538473606\n","Epoch [996/1000], Loss: 0.018942058086395264\n","Epoch [997/1000], Loss: 0.031793124973773956\n","Epoch [998/1000], Loss: 0.11385862529277802\n","Epoch [999/1000], Loss: 0.028287064284086227\n","Epoch [1000/1000], Loss: 0.018119366839528084\n"]}]},{"cell_type":"code","source":["# test on some inputs -- is perfect memory achieved"],"metadata":{"id":"eZDdGA1wPgkk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_word_ids= list(range(0, N))\n","X_test = X_train[test_word_ids]\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test)\n","\n","# Convert predictions to words\n","predicted_words = []\n","for output in outputs:\n","    predicted_word = ''\n","    for i in range(L):\n","        index = torch.argmax(output[i])\n","        if index == V-1:\n","            break\n","        predicted_word += index_to_letter[index.item()]\n","    predicted_words.append(predicted_word)\n","\n","\n","for i in range(len(test_word_ids)):\n","  w = words[test_word_ids[i]]\n","  wh = predicted_words[i]\n","  print(w,wh)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ErbVVkX4QkWT","executionInfo":{"status":"ok","timestamp":1715055870962,"user_tz":-330,"elapsed":398,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}},"outputId":"89359644-6b6f-411f-b0fd-cc339b2613fe"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["the the\n","and and\n","have have\n","that that\n","for for\n","you you\n","with with\n","say say\n","this this\n","they they\n","but but\n","his his\n","from from\n","not not\n","she she\n","as as\n","what what\n","their their\n","can can\n","who who\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"./detok_toy_trained_model.pth\")"],"metadata":{"id":"qzgIFalBVuik","executionInfo":{"status":"ok","timestamp":1715056689239,"user_tz":-330,"elapsed":373,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Get the weights of the RNN model\n","rnn_weights = model.rnn.state_dict()\n","\n","# Visualize the weights\n","for name, weight in rnn_weights.items():\n","    print(f\"Layer: {name}\")\n","    print()\n","\n","# Visualize the weights\n","for name, weight in rnn_weights.items():\n","    print(f\"Layer: {name}\")\n","    print(weight.shape)\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPrP04f1WNhh","executionInfo":{"status":"ok","timestamp":1715057925995,"user_tz":-330,"elapsed":392,"user":{"displayName":"soma dhavala","userId":"15308128232301337351"}},"outputId":"f89f093b-b3b0-473e-bee9-7d32145107fe"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer: weight_ih_l0\n","\n","Layer: weight_hh_l0\n","\n","Layer: bias_ih_l0\n","\n","Layer: bias_hh_l0\n","\n","Layer: weight_ih_l0\n","torch.Size([27, 20])\n","\n","Layer: weight_hh_l0\n","torch.Size([27, 27])\n","\n","Layer: bias_ih_l0\n","torch.Size([27])\n","\n","Layer: bias_hh_l0\n","torch.Size([27])\n","\n"]}]}]}