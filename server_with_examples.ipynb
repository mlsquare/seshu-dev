{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553eb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mlsquare/mergekit-mamba.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed584747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yashc/elephant/mamba-hf/src\n"
     ]
    }
   ],
   "source": [
    "cd mamba-hf/src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cff9f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ca917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a17c52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer,TrainingArguments\n",
    "from huggingface_hub import HfApi, ModelFilter\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import json\n",
    "from configuration_mamba import MambaConfig\n",
    "from modeling_mamba import MambaModel, MambaForCausalLM\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftMixedModel\n",
    "from transformers import DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54c6826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huseinzol05/dummy-mamba-1.4b\n",
      "mesolitica/mamba-1.4B-4096\n",
      "Q-bert/Mamba-130M\n",
      "Q-bert/Mamba-370M\n",
      "Q-bert/Mamba-790M\n",
      "Q-bert/Mamba-1B\n",
      "Q-bert/Mamba-3B\n",
      "s3nh/mamba-1.4b_dolly_instruction_polish\n",
      "ybelkada/test-axolotl-axolotlmambatrainer\n",
      "Q-bert/Mamba-3B-slimpj\n",
      "Q-bert/MambaHermes-3B\n",
      "ZySec-AI/Mamba-2.8B-CyberSec\n",
      "ayoubkirouane/Mamba-Chat-2.8B\n",
      "DeepMount00/Mamba-QA-ITA\n",
      "kuotient/mamba-ko-2.8b\n",
      "Trelis/mamba-2.8b-slimpj-bf16\n",
      "mjschock/mamba-130m\n",
      "mescarda/my_awesome_model\n",
      "DeepMount00/Mamba-QA-ITA-790m\n",
      "mjschock/mamba-370m\n",
      "mjschock/mamba-790m\n",
      "mjschock/mamba-1.4b\n",
      "ArthurZ/small-model\n",
      "ArthurZ/mamba-130m\n",
      "mlsquare/exp_causal_mamba\n",
      "mlsquare/mamba1\n",
      "mlsquare/mamba2\n",
      "mlsquare/mamba3\n",
      "ArthurZ/mamba-2.8b\n",
      "ArthurZ/mamba-370m\n",
      "ArthurZ/mamba-790m\n",
      "ArthurZ/mamba-1.4b\n",
      "ArthurZ/mamba-2.8b-slimpj\n",
      "mjschock/mamba-130m-ppo\n",
      "mlsquare/pico_mamba\n",
      "mlsquare/pico_seshu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mlsquare/exp_causal_mamba',\n",
       " 'mlsquare/mamba1',\n",
       " 'mlsquare/mamba2',\n",
       " 'mlsquare/mamba3',\n",
       " 'mlsquare/pico_mamba',\n",
       " 'mlsquare/pico_seshu']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from peft import PeftMixedModel\n",
    "def get_models_by_organization(org_id):\n",
    "    api = HfApi()\n",
    "    new_filter = ModelFilter(tags=\"mamba\")\n",
    "    models = api.list_models(filter=new_filter)\n",
    "    models_list = []\n",
    "    for i in models:\n",
    "        print(i.modelId)\n",
    "        if org_id in i.modelId:\n",
    "            models_list.append(i.modelId)\n",
    "    return models_list\n",
    "\n",
    "\n",
    "org_id = \"mlsquare\"\n",
    "models = get_models_by_organization(org_id)\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cc29641",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters = {\n",
    "    \"small\" : [\"mlsquare/mamba_130M_small_out_proj\", \"mlsquare/mamba_130M_small_dt_proj\", \"mlsquare/mamba_130M_small_x_proj\"],\n",
    "    \"large\" : [\"mlsquare/mamba_130M_large_x_dt_out_proj\"]\n",
    "}\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False): \n",
    "    lm_logits = model(inputs)[0]\n",
    "    labels = inputs.to(lm_logits.device)\n",
    "\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "    return lm_loss\n",
    "\n",
    "    \n",
    "def evaluation(data, model, tokenizer):\n",
    "    val = 0\n",
    "    for i in tqdm(data, desc=\"Evaluating\"):\n",
    "        value = tokenizer.encode(i['tgt'], return_tensors=\"pt\")\n",
    "        val += compute_loss(model, value)\n",
    "        \n",
    "    avg_loss = val / len(data)\n",
    "    print(\"LOSS: \", avg_loss)\n",
    "    return avg_loss\n",
    "\n",
    "def model_merge_large(adapters, model_path, data, tokenizer):\n",
    "    \n",
    "    model = MambaForCausalLM.from_pretrained(model_path)\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    model.load_adapter(adapters[\"large\"][0])\n",
    "    print(\"adapter merged\")\n",
    "    \n",
    "    result = evaluation(data, model, tokenizer)\n",
    "    return result\n",
    "    \n",
    "def model_merge_small(adapters, model_path, data, tokenizer):\n",
    "\n",
    "    base_model = MambaForCausalLM.from_pretrained(model_path)\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    peft_model = PeftMixedModel.from_pretrained(base_model, adapters[\"small\"][0])\n",
    "    peft_model.load_adapter(adapters[\"small\"][1], adapter_name=\"1\")\n",
    "    peft_model.load_adapter(adapters[\"small\"][2], adapter_name=\"2\")\n",
    "    peft_model.set_adapter([\"default\", \"1\", \"2\"])\n",
    "    print(\"adapter merged\")\n",
    "    \n",
    "    result = evaluation(data, peft_model, tokenizer)\n",
    "    return result\n",
    "    \n",
    "def create_JSON(value):\n",
    "    json_data = json.dumps(value, indent=4)\n",
    "    with open(f\"{value}\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "def get_data(data_path, fraction = 0.01):\n",
    "    data = load_dataset(data_path)['train'].shuffle()\n",
    "    data = data.select(list(range(int(len(data) * fraction))))\n",
    "    print(\"data fetched\")\n",
    "    return data\n",
    "def load_tokenizer(path):\n",
    "    return AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b26aac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fetched\n",
      "model loaded\n",
      "adapter merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████| 796/796 [00:36<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:  tensor(2.2215, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = get_data(mamba_130M_small[\"data\"])\n",
    "tokenizer = load_tokenizer(mamba_130M_small[\"tokenizer_path\"])\n",
    "result = model_merge_small(adapters, mamba_130M_small[\"model_path\"], data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf293f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fetched\n",
      "model loaded\n",
      "adapter merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████| 796/796 [00:37<00:00, 21.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:  tensor(2.2600, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = get_data(mamba_130M_large[\"data\"])\n",
    "tokenizer = load_tokenizer(mamba_130M_large[\"tokenizer_path\"])\n",
    "result = model_merge_small(adapters, mamba_130M_large[\"model_path\"], data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b79f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<model>-<PARAMS>-<AdapterComputation>-<target_modules>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d1cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_small = {\n",
    "    \"model_path\" : \"mlsquare/pico_seshu\",\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"adapter_path\" : \"mlsquare/mamba_130M_large_x_dt\",\n",
    "    \"data\" : \"mlsquare/samantar1per_cent_merged_with_train_val\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cba4e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_large = {\n",
    "    \"model_path\" : \"mlsquare/pico_seshu\",\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"adapter_path\" : \"mlsquare/mamba_130M_large_x_dt\",\n",
    "    \"data\" : \"mlsquare/samantar1per_cent_merged_with_train_val\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103ca6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "pose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
