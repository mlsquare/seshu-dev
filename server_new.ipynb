{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac800b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $TOKEN$ --add-to-git-credential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a569b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yashc/elephant/mamba-hf/src\n"
     ]
    }
   ],
   "source": [
    "cd mamba-hf/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9451cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer,TrainingArguments\n",
    "from huggingface_hub import HfApi, ModelFilter\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import json\n",
    "from configuration_mamba import MambaConfig\n",
    "from modeling_mamba import MambaModel, MambaForCausalLM\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftMixedModel\n",
    "from transformers import DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85d89b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inputs, return_outputs=False): \n",
    "    lm_logits = model(inputs)[0]\n",
    "    labels = inputs.to(lm_logits.device)\n",
    "\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "    return lm_loss\n",
    "\n",
    "\n",
    "def evaluation(data, model, tokenizer, batch_size=32, max_length = 1024):\n",
    "    num_samples = len(data)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Evaluating\"):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "            batch_data = data['tgt'][start_idx:end_idx]\n",
    "            inputs = [tokenizer.encode(datum, return_tensors=\"pt\",truncation=True, padding='max_length', max_length=max_length) for datum in batch_data]\n",
    "            input_ids = torch.cat(inputs, dim=0)\n",
    "            \n",
    "            loss = compute_loss(model, input_ids)\n",
    "            total_loss += loss.item() * (end_idx - start_idx)\n",
    "            \n",
    "    avg_loss = total_loss / num_samples\n",
    "    return avg_loss\n",
    "\n",
    "def model_merge(adapters, model_path, data, tokenizer):\n",
    "    base_model = MambaForCausalLM.from_pretrained(model_path)\n",
    "    print(\"model loaded\")\n",
    "    ls_count = 0\n",
    "    names = [\"default\"]\n",
    "    peft_model = PeftMixedModel.from_pretrained(base_model, adapters[ls_count])\n",
    "    ls_count += 1\n",
    "    while ls_count < len(adapters):\n",
    "        peft_model.load_adapter(adapters[ls_count], adapter_name=str(ls_count))\n",
    "        names.append(str(ls_count))\n",
    "        ls_count += 1\n",
    "        \n",
    "    peft_model.set_adapter(names)\n",
    "    peft_model = peft_model.merge_and_unload()\n",
    "    print(\"adapter merged\")\n",
    "    \n",
    "    result = evaluation(data, peft_model, tokenizer)\n",
    "    return result\n",
    "    \n",
    "def create_JSON(value):\n",
    "    json_data = json.dumps(value, indent=4)\n",
    "    with open(f\"{value}\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "def get_data(data_path, fraction = 0.01):\n",
    "    data = load_dataset(data_path)['train'].shuffle()\n",
    "    data = data.select(list(range(int(len(data) * fraction))))\n",
    "    print(\"data fetched\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22c6fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "    return loaded_data\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "def load_data(data_path):\n",
    "    data = load_dataset(data_path).shuffle()\n",
    "    return DatasetDict({\"train\":  data[\"train\"].select(list(range(int(len(data[\"train\"]) * 0.5)))), \n",
    "                        \"valid\":  data[\"valid\"].select(list(range(int(len(data[\"valid\"]) * 0.5))))} )\n",
    "def load_model(config):\n",
    "    config = make_config(config)\n",
    "    return MambaForCausalLM(config)\n",
    "\n",
    "def load_model_pretrained(config):\n",
    "    return MambaForCausalLM.from_pretrained(config)\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    return AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "def make_config(json):\n",
    "    config = MambaConfig(\n",
    "    vocab_size = json[\"vocab_size\"],\n",
    "    d_model = json[\"d_model\"],\n",
    "    d_conv = json[\"d_conv\"],\n",
    "    expand = json[\"expand\"],\n",
    "    conv_bias = json[\"conv_bias\"],\n",
    "    bias = json[\"bias\"],\n",
    "    n_layer = json[\"n_layer\"],\n",
    "    dt_rank = json[\"dt_rank\"],\n",
    "    pad_vocab_size_multiple = json[\"pad_vocab_size_multiple\"],\n",
    "    initializer_range = json[\"initializer_range\"],\n",
    "    )\n",
    "    return config\n",
    "\n",
    "# def split_data(data):\n",
    "#     train_size = int(len(data) * 0.8)\n",
    "#     valid_size = len(data) - train_size \n",
    "\n",
    "#     ds_train = data.select(list(range(train_size)))\n",
    "#     ds_valid = data.select(list(range(train_size, train_size + valid_size)))\n",
    "\n",
    "#     return DatasetDict({\"train\": ds_train, \"valid\": ds_valid})\n",
    "\n",
    "\n",
    "\n",
    "# def load_model_with_LoRA(model, target_modules):\n",
    "#     config = LoraConfig(\n",
    "#     target_modules = target_modules)\n",
    "#     m1 = get_peft_model(model, config)\n",
    "#     m1.print_trainable_parameters()\n",
    "#     m1.save_pretrained(\"./wts/adapter\")\n",
    "#     return m1\n",
    "\n",
    "def get_checkpoint_model(model_name):\n",
    "    def get_models_by_organization(org_id, model_name):\n",
    "        api = HfApi()\n",
    "        new_filter = ModelFilter(tags=\"mamba\")\n",
    "        models = api.list_models(filter=new_filter)\n",
    "\n",
    "        models_list = []\n",
    "        for i in models:\n",
    "            if (org_id in i.modelId):\n",
    "                print(i)\n",
    "                if((model_name in i.modelId)):\n",
    "                    return i.modelId\n",
    "        return False\n",
    "\n",
    "\n",
    "    org_id = \"mlsquare\"\n",
    "    return get_models_by_organization(org_id, model_name)\n",
    "\n",
    "def make_config(json):\n",
    "    config = MambaConfig(\n",
    "    vocab_size = json[\"vocab_size\"],\n",
    "    d_model = json[\"d_model\"],\n",
    "    d_conv = json[\"d_conv\"],\n",
    "    expand = json[\"expand\"],\n",
    "    conv_bias = json[\"conv_bias\"],\n",
    "    bias = json[\"bias\"],\n",
    "    n_layer = json[\"n_layer\"],\n",
    "    dt_rank = json[\"dt_rank\"],\n",
    "    pad_vocab_size_multiple = json[\"pad_vocab_size_multiple\"],\n",
    "    initializer_range = json[\"initializer_range\"],\n",
    "    )\n",
    "    return config\n",
    "\n",
    "class MambaTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):        \n",
    "        input_ids = inputs.pop(\"input_ids\")\n",
    "        lm_logits = model(input_ids)[0]\n",
    "        labels = input_ids.to(lm_logits.device)\n",
    "        \n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "        return lm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e5ed93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seshu:\n",
    "    def __init__(self,adapters, config_file, train_args = False):\n",
    "        self.adapters = load_json(adapters)\n",
    "        self.config_data = load_json(config_file)\n",
    "        if train_args:\n",
    "            self.train_args = train_args\n",
    "        else:\n",
    "            self.train_args = TrainingArguments(\n",
    "                                output_dir=\"mamba\",\n",
    "                                per_device_train_batch_size=1,\n",
    "                                per_device_eval_batch_size=1,\n",
    "                                num_train_epochs=4,\n",
    "                                weight_decay=0.1,\n",
    "                                lr_scheduler_type=\"cosine\",\n",
    "                                learning_rate=5e-4,\n",
    "                                fp16=False,\n",
    "                            )\n",
    "        self.tokenizer = load_tokenizer(self.config_data[\"tokenizer_path\"])\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        outputs = self.tokenizer(\n",
    "            data[\"tgt\"],\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_length=True,\n",
    "        )\n",
    "        input_batch = []\n",
    "        for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "            if length != 0:\n",
    "                input_batch.append(input_ids)\n",
    "        return {\"input_ids\": input_batch}\n",
    "        \n",
    "        \n",
    "    def pretrain(self):\n",
    "#         model_config = make_config(self.config_data)\n",
    "        if get_checkpoint_model(self.config_data[\"upload_path\"]):\n",
    "            model = load_model_pretrained(self.config_data[\"upload_path\"])\n",
    "        else:\n",
    "            model = load_model(self.config_data)\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        data = load_data(self.config_data[\"data\"])\n",
    "        tokenized_data = data.map(self.tokenize, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "        trainer = MambaTrainer( model=model, tokenizer=self.tokenizer, args=self.train_args, data_collator=data_collator,\n",
    "                                train_dataset=tokenized_data[\"train\"], eval_dataset=tokenized_data[\"valid\"])\n",
    "        trainer.train()\n",
    "\n",
    "    def model_merge_eval(self, model_path, type_config = \"small\", data = \"mlsquare/SERVER_samantar_mixed_val\"):\n",
    "        adapters = self.adapters[type_config]\n",
    "        data = get_data(data)\n",
    "        tokenizer = self.tokenizer\n",
    "        result = model_merge(adapters, model_path, data, tokenizer)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "037ab556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fetched\n",
      "model loaded\n",
      "adapter merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.901081181779692"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seshu(\"adapters.json\", \"model_parameters_lora.json\")\n",
    "model.model_merge_eval( \"mlsquare/pico_seshu_test\", type_config = \"large\", data = \"mlsquare/SERVER_samantar_mixed_val\")\n",
    "# model.train_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CONFIG GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9894721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_path': 'mlsquare/pico_seshu_test', 'tokenizer_path': 'google/byt5-large', 'adapter_path': 'mlsquare/mamba_pico_small_dt_proj', 'data': 'mlsquare/CLIENT_samantar_mixed_train_val', 'target_modules': ['model.layers.3.dt_proj']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"model_path\" : \"mlsquare/pico_seshu_test\",\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"adapter_path\" : \"mlsquare/mamba_pico_small_dt_proj\",\n",
    "    \"data\" : \"mlsquare/CLIENT_samantar_mixed_train_val\",\n",
    "    \"target_modules\" : [\"model.layers.3.dt_proj\"]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to JSON format\n",
    "json_data = json.dumps(data, indent=4)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"model_parameters_lora.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "    \n",
    "with open(\"model_parameters_lora.json\", \"r\") as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "\n",
    "# Print the loaded data\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab5bd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 20000, 'd_state': 16, 'd_model': 2560, 'd_conv': 4, 'expand': 2, 'conv_bias': True, 'bias': False, 'n_layer': 64, 'pad_vocab_size_multiple': 8, 'dt_rank': 'auto', 'initializer_range': 0.02, 'tokenizer_path': 'google/byt5-large', 'upload_path': 'mlsquare/130M_Seshu', 'data': 'mlsquare/CLIENT_samantar_mixed_train_val'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"vocab_size\":20000,\n",
    "    \"d_state\":16,\n",
    "    \"d_model\":2560,\n",
    "    \"d_conv\":4,\n",
    "    \"expand\":2,\n",
    "    \"conv_bias\":True,\n",
    "    \"bias\":False,\n",
    "    \"n_layer\":64,\n",
    "    \"pad_vocab_size_multiple\":8,\n",
    "    \"dt_rank\": \"auto\",\n",
    "    \"initializer_range\":0.02,\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"upload_path\" : \"mlsquare/130M_Seshu\",\n",
    "    \"data\" : \"mlsquare/CLIENT_samantar_mixed_train_val\"\n",
    "}\n",
    "\n",
    "# Convert the dictionary to JSON format\n",
    "json_data = json.dumps(data, indent=4)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"model_parameters.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "    \n",
    "with open(\"model_parameters.json\", \"r\") as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "\n",
    "# Print the loaded data\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "958888c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'small': ['mlsquare/mamba_pico_small_out_proj', 'mlsquare/mamba_pico_small_dt_proj', 'mlsquare/mamba_pico_small_x_proj'], 'large': ['mlsquare/mamba_pico_large_x_dt_out_proj']}\n"
     ]
    }
   ],
   "source": [
    "data = {  \"small\" : [\"mlsquare/mamba_pico_small_out_proj\", \"mlsquare/mamba_pico_small_dt_proj\", \"mlsquare/mamba_pico_small_x_proj\"],\n",
    "           \"large\" : [\"mlsquare/mamba_pico_large_x_dt_out_proj\"]\n",
    "                }\n",
    "\n",
    "json_data = json.dumps(data, indent=4)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"adapters.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "    \n",
    "with open(\"adapters.json\", \"r\") as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "\n",
    "# Print the loaded data\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683e62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "pose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
